--- 
title: "Dataset compare between models"
subtitle: "Machine Learning"
author: "Marta Ferreira e Pedro Magalh√£es"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
geometry: margin=2.5cm
documentclass: article
link-citations: yes
pdf-cover-image: theme/images/cover.pdf
cover-image: theme/images/cover.pdf
bibliography: [book.bib, packages.bib]
biblio-style: apalike
---

\newpage

# Introduction {-}

This projects aims at comparing the performance of several Machine Learning models (ML models) under different data contexts.

To achieve our goal we stacked against each other pairs of different models using Synthetic Datasets represents often polarizing and extreme situations and therefore exposing the main "decision" characteristics of each model.

The focus will be solely on classification problems and models will be compared againts each other using Accuracy and Area under the Roc Curve (AUC ROC) as metrics. Since we have full control over the dataset distributuion, the optimal Bayes Boundary willbe used as baseline


## Project structure {-}

This project is organized in the following way:

1. An introduction containing general assumptions and how to reproduce the results,

2. A chapter for each model pair containing the sythetic data rules, the bayes optimal boundary (BOB), a small models explanation and rational for each dataset, model fit and metrics as well as the prediction area and discussion of the results,

3. An overall conclusion


## Approaches and Assumptions {-}

Throughtout this project the following assumptions were made and approaches were used:

- Dependent variables (target) are of date type `factor` and all Independent variables (features) are of type `numeric`,

- Since the datasets are syntheticly generated, no pre-processing was made,

- Datasets were built using statistical distributions and/or classification rules which may have no resemblance to reality,

- To the extant it is possible a dataset was created using a binomial categorical variable and a two features,

- For the sake of molde comparing, the default hyperparameter value were used. This are provided by `Parsnip R package` without post-processing and hyperparameter optimzation. It is possible, although unlikely, that a different package or under different hyperparameters could result different conclusions,

- For calculating metrics a crossvalidatios with 10 folds and no repetition was used.


## How to run and reproduce the conclusions {-}

## Environment Info {-}

```{r}

sessionInfo()

```



\newpage
