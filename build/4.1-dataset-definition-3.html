<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="4.1 Dataset definition | Dataset compare between models" />
<meta property="og:type" content="book" />
<meta property="og:image" content="/theme/images/cover.pdf" />



<meta name="author" content="Marta Ferreira e Pedro MagalhÃ£es" />

<meta name="date" content="2022-06-07" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="4.1 Dataset definition | Dataset compare between models">

<title>4.1 Dataset definition | Dataset compare between models</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="_theme/style.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#introduction">Introduction</a>
<ul>
<li><a href="project-structure.html#project-structure">Project structure</a></li>
<li><a href="approaches-and-assumptions.html#approaches-and-assumptions">Approaches and Assumptions</a></li>
<li><a href="how-to-run-and-reproduce-the-conclusions.html#how-to-run-and-reproduce-the-conclusions">How to run and reproduce the conclusions</a></li>
<li><a href="environment-info.html#environment-info">Environment Info</a></li>
</ul></li>
<li class="has-sub"><a href="1-logistic-regression-vs-nearest-neighbour.html#logistic-regression-vs-nearest-neighbour"><span class="toc-section-number">1</span> Logistic regression vs Nearest Neighbour</a>
<ul>
<li><a href="1.1-dataset-definition.html#dataset-definition"><span class="toc-section-number">1.1</span> Dataset definition</a></li>
<li><a href="1.2-model-fitting.html#model-fitting"><span class="toc-section-number">1.2</span> Model fitting</a></li>
<li><a href="1.3-compare-results.html#compare-results"><span class="toc-section-number">1.3</span> Compare results</a></li>
<li><a href="1.4-conclusion.html#conclusion"><span class="toc-section-number">1.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="2-mlp-vs-knn.html#mlp-vs-knn"><span class="toc-section-number">2</span> MLP vs knn</a>
<ul>
<li><a href="2.1-dataset-definition-1.html#dataset-definition-1"><span class="toc-section-number">2.1</span> Dataset definition</a></li>
<li><a href="2.2-model-fitting-1.html#model-fitting-1"><span class="toc-section-number">2.2</span> Model fitting</a></li>
<li><a href="2.3-compare-results-1.html#compare-results-1"><span class="toc-section-number">2.3</span> Compare results</a></li>
<li><a href="2.4-conclusion-1.html#conclusion-1"><span class="toc-section-number">2.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="3-mlp-relu-vs-mlp-sigmoid.html#mlp-relu-vs-mlp-sigmoid"><span class="toc-section-number">3</span> 11. MLP ReLu vs MLP sigmoid</a>
<ul>
<li><a href="3.1-dataset-definition-2.html#dataset-definition-2"><span class="toc-section-number">3.1</span> Dataset definition</a></li>
<li><a href="3.2-model-fitting-2.html#model-fitting-2"><span class="toc-section-number">3.2</span> Model fitting</a></li>
<li><a href="3.3-compare-results-2.html#compare-results-2"><span class="toc-section-number">3.3</span> Compare results</a></li>
<li><a href="3.4-conclusion-2.html#conclusion-2"><span class="toc-section-number">3.4</span> Conclusion</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
<li class="has-sub"><a href="4-linear-discriminante-analysis-vs-decision-tree.html#linear-discriminante-analysis-vs-decision-tree"><span class="toc-section-number">4</span> Linear Discriminante Analysis vs Decision tree</a>
<ul>
<li><a href="4.1-dataset-definition-3.html#dataset-definition-3"><span class="toc-section-number">4.1</span> Dataset definition</a></li>
<li><a href="4.2-model-fitting-3.html#model-fitting-3"><span class="toc-section-number">4.2</span> Model fitting</a></li>
<li><a href="4.3-compare-results-3.html#compare-results-3"><span class="toc-section-number">4.3</span> Compare results</a></li>
<li><a href="4.4-conclusion-3.html#conclusion-3"><span class="toc-section-number">4.4</span> Conclusion</a></li>
<li><a href="4.5-dataset-definition-4.html#dataset-definition-4"><span class="toc-section-number">4.5</span> Dataset definition</a></li>
<li class="has-sub"><a href="4.6-dataset-definition-5.html#dataset-definition-5"><span class="toc-section-number">4.6</span> Dataset definition</a>
<ul>
<li><a href="4.6-dataset-definition-5.html#lda"><span class="toc-section-number">4.6.1</span> LDA</a></li>
<li><a href="4.6-dataset-definition-5.html#qda"><span class="toc-section-number">4.6.2</span> QDA</a></li>
</ul></li>
<li><a href="4.7-compare-results-4.html#compare-results-4"><span class="toc-section-number">4.7</span> Compare results</a></li>
</ul></li>
<li class="has-sub"><a href="5-linear-discriminante-analysis-vs-logistic-regression.html#linear-discriminante-analysis-vs-logistic-regression"><span class="toc-section-number">5</span> Linear Discriminante Analysis vs Logistic Regression</a>
<ul>
<li><a href="5.1-dataset-definition-6.html#dataset-definition-6"><span class="toc-section-number">5.1</span> Dataset definition</a></li>
<li><a href="5.2-model-fitting-4.html#model-fitting-4"><span class="toc-section-number">5.2</span> Model fitting</a></li>
<li><a href="5.3-compare-results-5.html#compare-results-5"><span class="toc-section-number">5.3</span> Compare results</a></li>
<li><a href="5.4-conclusion-4.html#conclusion-4"><span class="toc-section-number">5.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="6-nearest-neighbour-vs.-decision-tree.html#nearest-neighbour-vs.-decision-tree"><span class="toc-section-number">6</span> Nearest neighbour vs.Â decision tree</a>
<ul>
<li><a href="6.1-model-fitting-5.html#model-fitting-5"><span class="toc-section-number">6.1</span> Model fitting</a></li>
<li><a href="6.2-compare-results-6.html#compare-results-6"><span class="toc-section-number">6.2</span> Compare results</a></li>
<li><a href="6.3-conclusion-5.html#conclusion-5"><span class="toc-section-number">6.3</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="7-decision-tree-vs-tree-boosting.html#decision-tree-vs-tree-boosting"><span class="toc-section-number">7</span> Decision tree vs tree boosting</a>
<ul>
<li><a href="7.1-dataset-definition-7.html#dataset-definition-7"><span class="toc-section-number">7.1</span> Dataset definition</a></li>
<li><a href="7.2-model-fitting-6.html#model-fitting-6"><span class="toc-section-number">7.2</span> Model fitting</a></li>
<li><a href="7.3-compare-results-7.html#compare-results-7"><span class="toc-section-number">7.3</span> Compare results</a></li>
<li><a href="7.4-conclusion-6.html#conclusion-6"><span class="toc-section-number">7.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="8-svm-radial-vs-svm-linear.html#svm-radial-vs-svm-linear"><span class="toc-section-number">8</span> SVM Radial vs SVM linear</a>
<ul>
<li><a href="8.1-dataset-definition-8.html#dataset-definition-8"><span class="toc-section-number">8.1</span> Dataset definition</a></li>
<li><a href="8.2-model-fitting-7.html#model-fitting-7"><span class="toc-section-number">8.2</span> Model fitting</a></li>
<li><a href="8.3-compare-results-8.html#compare-results-8"><span class="toc-section-number">8.3</span> Compare results</a></li>
<li><a href="8.4-conclusion-7.html#conclusion-7"><span class="toc-section-number">8.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="9-svm-radial-vs-svm-polynomial.html#svm-radial-vs-svm-polynomial"><span class="toc-section-number">9</span> SVM Radial vs SVM Polynomial</a>
<ul>
<li><a href="9.1-dataset-definition-9.html#dataset-definition-9"><span class="toc-section-number">9.1</span> Dataset definition</a></li>
<li><a href="9.2-model-fitting-8.html#model-fitting-8"><span class="toc-section-number">9.2</span> Model fitting</a></li>
<li><a href="9.3-compare-results-9.html#compare-results-9"><span class="toc-section-number">9.3</span> Compare results</a></li>
<li><a href="9.4-conclusion-8.html#conclusion-8"><span class="toc-section-number">9.4</span> Conclusion</a></li>
</ul></li>
<li class="has-sub"><a href="10-mlp-vs-knn-1.html#mlp-vs-knn-1"><span class="toc-section-number">10</span> MLP vs knn</a>
<ul>
<li><a href="10.1-dataset-definition-10.html#dataset-definition-10"><span class="toc-section-number">10.1</span> Dataset definition</a></li>
<li><a href="10.2-model-fitting-9.html#model-fitting-9"><span class="toc-section-number">10.2</span> Model fitting</a></li>
<li><a href="10.3-compare-results-10.html#compare-results-10"><span class="toc-section-number">10.3</span> Compare results</a></li>
<li><a href="10.4-conclusion-9.html#conclusion-9"><span class="toc-section-number">10.4</span> Conclusion</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="dataset-definition-3" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Dataset definition</h2>
<p>Similarly to Logistic regression, Linear Discriminant Analysis (LDA) estimates each class by modeling the conditional distribution <span class="math inline">\(Pr(G = 1 | X)\)</span>, but using a different approach. While linear regression uses the <em>Logitisc function</em> for this purpose, LDA assumes each observation is drawan from a multivariated normal distribution with similar covariance and uses this distribution to model the conditional distribution. Therefore, on many ocasions, the output of both Logistic and LDA will be quite similar. <em><span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-hastie_09_elements-of.statistical-learning" role="doc-biblioref">2009</a>)</span> in their experience, this models give very similar results.</em></p>
<p>Tree based methods take a very different approach to the problem. They involve <em>stratiying</em> or <em>segmenting</em> the predictor space into a number of simple regions.(see <span class="citation">James et al. (<a href="#ref-James2013" role="doc-biblioref">2013</a>)</span> pag-303). A metric like the mean or median is then used as predictor for each region or cut.</p>
<p>This oposing technics have pronounced characteristics making then appropriate to dataset with specific characteristics. Whenever the classes are based on very pronounced cuts or classes, tree based approaches and their rule based classification will tend to fit best. On the other hand, on its simple form as decision tree, they tend to perform worse then tradional parametric conterparts as LDA when the classification border follows a clear distribution.</p>
<p>Therefore, we defined the comparing datasets as follows:</p>
<ol style="list-style-type: decimal">
<li>A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs above <span class="math inline">\(X_{1} - X_{2} = 0\)</span> are classified as <strong>1</strong> and <strong>0</strong> if below. In other words, we defined a hyperplane through the data and defined each class based on each point position towards that hyper plane.</li>
</ol>
<blockquote>
<p><span class="citation">James et al. (<a href="#ref-James2013" role="doc-biblioref">2013</a>)</span> In a p-dimensional space, a <em>hyperplane</em> is a flat affline subspace of dimension p-1. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace-in other words, a line. In two dimensions the hyperplane is defined as</p>
</blockquote>
<p><span class="math display">\[\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} = 0\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs below a specific treshold (x1,x2) are classified as 1 and 0 for the rest. Considering for example x1 as weight and x2 as height would be equivalent as classifying every combination below a x1,x2 as âchildrenâ or âunderweightâ.</li>
</ol>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="4.1-dataset-definition-3.html#cb12-1" aria-hidden="true" tabindex="-1"></a>decision_fun_linear <span class="ot">&lt;-</span> <span class="cf">function</span>(x1, x2){</span>
<span id="cb12-2"><a href="4.1-dataset-definition-3.html#cb12-2" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(x2 <span class="sc">&gt;=</span> x1, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb12-3"><a href="4.1-dataset-definition-3.html#cb12-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(res)</span>
<span id="cb12-4"><a href="4.1-dataset-definition-3.html#cb12-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-5"><a href="4.1-dataset-definition-3.html#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="4.1-dataset-definition-3.html#cb12-6" aria-hidden="true" tabindex="-1"></a>dataset_linear <span class="ot">&lt;-</span> <span class="fu">dataset_gen_unif</span>(<span class="at">class_fun =</span> decision_fun_linear, <span class="at">size =</span> <span class="dv">1000</span>)</span>
<span id="cb12-7"><a href="4.1-dataset-definition-3.html#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="4.1-dataset-definition-3.html#cb12-8" aria-hidden="true" tabindex="-1"></a>decision_fun_normal <span class="ot">&lt;-</span> <span class="cf">function</span>(x1, x2){</span>
<span id="cb12-9"><a href="4.1-dataset-definition-3.html#cb12-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-10"><a href="4.1-dataset-definition-3.html#cb12-10" aria-hidden="true" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">6</span>,<span class="dv">6</span>)</span>
<span id="cb12-11"><a href="4.1-dataset-definition-3.html#cb12-11" aria-hidden="true" tabindex="-1"></a>  cvar <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.5</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.5</span>), <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-12"><a href="4.1-dataset-definition-3.html#cb12-12" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> mvtnorm<span class="sc">::</span><span class="fu">pmvnorm</span>(<span class="fu">c</span>(x1,x2), <span class="at">mean =</span> mu, <span class="at">sigma =</span> cvar)</span>
<span id="cb12-13"><a href="4.1-dataset-definition-3.html#cb12-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb12-14"><a href="4.1-dataset-definition-3.html#cb12-14" aria-hidden="true" tabindex="-1"></a>  res <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(p[<span class="dv">1</span>] <span class="sc">&lt;=</span> <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb12-15"><a href="4.1-dataset-definition-3.html#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(res)</span>
<span id="cb12-16"><a href="4.1-dataset-definition-3.html#cb12-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-17"><a href="4.1-dataset-definition-3.html#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="4.1-dataset-definition-3.html#cb12-18" aria-hidden="true" tabindex="-1"></a>dataset_dtree <span class="ot">&lt;-</span> <span class="fu">dataset_gen_unif</span>(<span class="at">class_fun =</span> decision_fun_normal)</span>
<span id="cb12-19"><a href="4.1-dataset-definition-3.html#cb12-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-20"><a href="4.1-dataset-definition-3.html#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="4.1-dataset-definition-3.html#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(</span>
<span id="cb12-22"><a href="4.1-dataset-definition-3.html#cb12-22" aria-hidden="true" tabindex="-1"></a>  dataset_dtree<span class="sc">$</span>border_plot <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Segmented datase&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>), </span>
<span id="cb12-23"><a href="4.1-dataset-definition-3.html#cb12-23" aria-hidden="true" tabindex="-1"></a>  dataset_linear<span class="sc">$</span>border_plot <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Linear border&quot;</span>, <span class="at">color =</span> <span class="st">&quot;&quot;</span>), </span>
<span id="cb12-24"><a href="4.1-dataset-definition-3.html#cb12-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrow =</span> <span class="dv">1</span>,</span>
<span id="cb12-25"><a href="4.1-dataset-definition-3.html#cb12-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">top =</span> <span class="st">&quot;Synthetic Generated Datasets&quot;</span>,</span>
<span id="cb12-26"><a href="4.1-dataset-definition-3.html#cb12-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">bottom =</span> grid<span class="sc">::</span><span class="fu">textGrob</span>(</span>
<span id="cb12-27"><a href="4.1-dataset-definition-3.html#cb12-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;Dashed line represent optimal bayes decision boundary&quot;</span>,</span>
<span id="cb12-28"><a href="4.1-dataset-definition-3.html#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="at">gp =</span> grid<span class="sc">::</span><span class="fu">gpar</span>(<span class="at">fontface =</span> <span class="dv">3</span>, <span class="at">fontsize =</span> <span class="dv">9</span>)</span>
<span id="cb12-29"><a href="4.1-dataset-definition-3.html#cb12-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-30"><a href="4.1-dataset-definition-3.html#cb12-30" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-44-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-hastie_09_elements-of.statistical-learning" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer. <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">http://www-stat.stanford.edu/~tibs/ElemStatLearn/</a>.
</div>
<div id="ref-James2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer. <a href="https://faculty.marshall.usc.edu/gareth-james/ISL/">https://faculty.marshall.usc.edu/gareth-james/ISL/</a>.
</div>
</div>
<p style="text-align: center;">
<a href="4-linear-discriminante-analysis-vs-decision-tree.html"><button class="btn btn-default">Previous</button></a>
<a href="4.2-model-fitting-3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
