[["index.html", "Dataset compare between models Machine Learning Introduction Project structure Approaches and Assumptions How to run and reproduce the conclusions", " Dataset compare between models Machine Learning Marta Ferreira e Pedro Magalh√£es 18/04/2022 Introduction This projects aims at comparing the performance of several Machine Learning models (ML models) under different data contexts. To achieve our goal we stacked against each other pairs of different models using Synthetic Datasets represents often polarizing and extreme situations and therefore exposing the main decision characteristics of each model. The focus will be solely on classification problems and models will be compared againts each other using Accuracy and Area under the Roc Curve (AUC ROC) as metrics. Since we have full control over the dataset distributuion, the optimal Bayes Boundary willbe used as baseline Project structure This project is organized in the following way: An introduction containing general assumptions and how to reproduce the results, A chapter for each model pair containing the sythetic data rules, the bayes optimal boundary (BOB), a small models explanation and rational for each dataset, model fit and metrics as well as the prediction area and discussion of the results, An overall conclusion Approaches and Assumptions Throughtout this project the following assumptions were made and approaches were used: Dependent variables (target) are of date type factor and all Independent variables (features) are of type numeric, Since the datasets are syntheticly generated, no pre-processing was made, Datasets were built using statistical distributions and/or classification rules which may have no resemblance to reality, To the extant it is possible a dataset was created using a binomial categorical variable and a two features, For the sake of molde comparing, the default hyperparameter value were used. This are provided by Parsnip R package without post-processing and hyperparameter optimzation. It is possible, although unlikely, that a different package or under different hyperparameters could result different conclusions, For calculating metrics a crossvalidatios with 10 folds and no repetition was used. How to run and reproduce the conclusions "],["logistic-vs-knn.html", "1 logistic vs knn 1.1 Dataset definition 1.2 Model fitting 1.3 Compare results 1.4 Conclusion", " 1 logistic vs knn 1.1 Dataset definition When comparing a Logistic regression model agains a Nearest Neighbour model we are comparing a highly biased and a very flexible approach. While a logostic regression assumes a linear border between both classes, NN makes no assumptions and and relies on local information (by k neighbours) to predict a class. Given the base assumption of a linear boundary by logistic regression, a model whose border differs, substantially from a line we expect will performe badly. On the other hand the lack of linearity wont an issue for knn. Therefore, we built two datasets with 1000 observations from a Uniform Distribution, one with a classification provided by a linear model of \\(X1 &gt;= X2\\) and a narrow quadratic function \\(abs(1,2 * X_{1} - 5)^2 + 2\\). decision_fun_linear &lt;- function(x1, x2){ res &lt;- ifelse(x2 &gt;= x1, 1, 0) return(res) } decision_fun_quadratic &lt;- function(x1, x2){ res &lt;- ifelse(x2 &gt;= abs( (1.2 * x1 - 5)^2 + 2 ), 1, 0) return(res) } dataset_linear &lt;- dataset_gen_unif(class_fun = decision_fun_linear, size = 1000) dataset_quadratic &lt;- dataset_gen_unif(class_fun = decision_fun_quadratic, size = 1000) grid.arrange( dataset_linear$border_plot, dataset_quadratic$border_plot, nrow = 1, top = &quot;Synthetic Generated Datasets&quot;, bottom = grid::textGrob( &quot;Dashed line represent optimal bayes decision border&quot;, gp = grid::gpar(fontface = 3, fontsize = 9) ) ) 1.2 Model fitting Each dataset was divided on training and test dataset using a 80/20 split. # 0. Separate test and train data_linear &lt;- dataset_linear$dataset data_linear$g &lt;- factor(data_linear$g) split &lt;- initial_split(data_linear, prop = 0.8) train_data_linear &lt;- training(split) test_data_linear &lt;- testing(split) data_quadratic &lt;- dataset_quadratic$dataset data_quadratic$g &lt;- factor(data_quadratic$g) split &lt;- initial_split(data_quadratic, prop = 0.8) train_data_quadratic &lt;- training(split) test_data_quadratic &lt;- testing(split) 1.2.1 Logistic regression ## Create workflow ### Logistic regression ------------------------- # 1. specify the model logistic_reg_glm_spec &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% set_engine(&#39;glm&#39;, family = &quot;binomial&quot;) # 2. preprocessing preprocess &lt;- recipe(g ~ x1 + x2 , data = train_data_linear) # 3, Buildworkflow logit_wflow &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(preprocess) # 4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) folds_linear &lt;- vfold_cv(train_data_linear, v = 10) folds_quadratic &lt;- vfold_cv(train_data_quadratic, v = 10) logit_metrics_linear &lt;- logit_wflow %&gt;% fit_resamples(folds_linear, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold02: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold03: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold04: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold05: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold06: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold07: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold08: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold09: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold10: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... logit_metrics_quadratic &lt;- logit_wflow %&gt;% fit_resamples(folds_quadratic, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; # 5. Performance metrics over the validation set logit_metrics_linear &lt;- collect_metrics(logit_metrics_linear, summarize = FALSE) logit_metrics_quadratic &lt;- collect_metrics(logit_metrics_quadratic, summarize = FALSE) # 6. Fits final model logit_linear_fit &lt;- logit_wflow %&gt;% fit(train_data_linear) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred logit_linear_model &lt;- extract_fit_parsnip(logit_linear_fit) logit_quadratic_fit &lt;- logit_wflow %&gt;% fit(train_data_quadratic) logit_quadratic_model &lt;- extract_fit_parsnip(logit_quadratic_fit) 1.2.2 KNN ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 1.3 Compare results The plots below show the resulting decision bondaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 1.4 Conclusion Given is simplicity and explicit difference between each class, both model perform very well on a dataset with a clear linear bondary. That is visible on both the training and test dataset although with an edge towards the logistic regression. It is when the boundary aliviates the linearity condition that knn really outshines the logistic output. It is important to notice that given the fact that the data derive from such strong definitions, it lakes randomness and therefore its not easy to identify signs of overfitting. "],["linear-discriminante-analysis-vs-decision-tree.html", "2 Linear Discriminante Analysis vs Decision tree 2.1 Dataset definition 2.2 Model fitting 2.3 Decision tree 2.4 Compare results 2.5 Conclusion", " 2 Linear Discriminante Analysis vs Decision tree 2.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 2.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. # 0. Separate test and train data_lda &lt;- dataset_lda$dataset data_lda$g &lt;- factor(data_lda$g) split &lt;- initial_split(data_lda, prop = 0.8) train_data_lda &lt;- training(split) test_data_lda &lt;- testing(split) data_dtree &lt;- dataset_dtree$dataset data_dtree$g &lt;- factor(data_dtree$g) split &lt;- initial_split(data_dtree, prop = 0.8) train_data_dtree &lt;- training(split) test_data_dtree &lt;- testing(split) 2.2.1 LDA Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Create workflow ### LDA regression ------------------------- # 1. specify the model discrim_linear_MASS_spec &lt;- discrim_linear() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;MASS&quot;) # 2. preprocessing preprocess &lt;- recipe(g ~ x1 + x2 , data = train_data_lda) # 3, Buildworkflow lda_wflow &lt;- workflow() %&gt;% add_model(discrim_linear_MASS_spec) %&gt;% add_recipe(preprocess) # 4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) folds_lda &lt;- vfold_cv(train_data_lda, v = 10) folds_dtree &lt;- vfold_cv(train_data_dtree, v = 10) lda_metrics_dataset_lda &lt;- lda_wflow %&gt;% fit_resamples(folds_lda, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; lda_metrics_dataset_dtree &lt;- lda_wflow %&gt;% fit_resamples(folds_dtree, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; # 5. Performance metrics over the validation set lda_metrics_dataset_lda &lt;- collect_metrics(lda_metrics_dataset_lda, summarize = FALSE) lda_metrics_dataset_dtree &lt;- collect_metrics(lda_metrics_dataset_dtree, summarize = FALSE) # 6. Fits final model lda_ldaFit &lt;- lda_wflow %&gt;% fit(train_data_lda) lda_ldaModel &lt;- extract_fit_parsnip(lda_ldaFit) lda_dtreeFit &lt;- lda_wflow %&gt;% fit(train_data_dtree) lda_dtreeModel &lt;- extract_fit_parsnip(lda_dtreeFit) 2.3 Decision tree ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 2.4 Compare results The plots below show the resulting decision bondaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 2.5 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["linear-discriminante-analysis-vs-decision-tree-1.html", "3 Linear Discriminante Analysis vs Decision tree", " 3 Linear Discriminante Analysis vs Decision tree "],["linear-discriminante-analysis-vs-logistic-regression.html", "4 Linear Discriminante Analysis vs Logistic Regression 4.1 Dataset definition 4.2 Model fitting 4.3 Compare results 4.4 Conclusion", " 4 Linear Discriminante Analysis vs Logistic Regression 4.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 4.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold02: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold03: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold04: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold05: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold06: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold07: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold08: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold09: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## ! Fold10: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fit: fitted probabi... ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 4.3 Compare results The plots below show the resulting decision boundaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 4.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["decision-tree-vs-tree-boosting.html", "5 Decision tree vs tree boosting 5.1 Dataset definition 5.2 Model fitting 5.3 Compare results 5.4 Conclusion", " 5 Decision tree vs tree boosting 5.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 5.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 5.3 Compare results The plots below show the resulting decision boundaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 5.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["svm-radial-vs-svm-linear.html", "6 SVM Radial vs SVM linear 6.1 Dataset definition 6.2 Model fitting 6.3 Compare results 6.4 Conclusion", " 6 SVM Radial vs SVM linear 6.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 6.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## maximum number of iterations reached 0.0006068825 -0.0006062308 6.3 Compare results The plots below show the resulting decision boundaries ## Warning: stat_contour(): Zero contours were generated ## Warning in min(x): no non-missing arguments to min; returning Inf ## Warning in max(x): no non-missing arguments to max; returning -Inf The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 6.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
