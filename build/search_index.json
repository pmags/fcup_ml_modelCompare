[["index.html", "Dataset compare between models Machine Learning Introduction Project structure Approaches and Assumptions How to run and reproduce the conclusions Environment Info", " Dataset compare between models Machine Learning Marta Ferreira e Pedro Magalh√£es 2022-06-04 Introduction This projects aims at comparing the performance of several Machine Learning models (ML models) under different data contexts. To achieve our goal we stacked against each other pairs of different models using Synthetic Datasets represents often polarizing and extreme situations and therefore exposing the main decision characteristics of each model. The focus will be solely on classification problems and models will be compared againts each other using Accuracy and Area under the Roc Curve (AUC ROC) as metrics. Since we have full control over the dataset distributuion, the optimal Bayes Boundary willbe used as baseline Project structure This project is organized in the following way: An introduction containing general assumptions and how to reproduce the results, A chapter for each model pair containing the sythetic data rules, the bayes optimal boundary (BOB), a small models explanation and rational for each dataset, model fit and metrics as well as the prediction area and discussion of the results, An overall conclusion Approaches and Assumptions Throughtout this project the following assumptions were made and approaches were used: Dependent variables (target) are of date type factor and all Independent variables (features) are of type numeric, Since the datasets are syntheticly generated, no pre-processing was made, Datasets were built using statistical distributions and/or classification rules which may have no resemblance to reality, To the extant it is possible a dataset was created using a binomial categorical variable and a two features, For the sake of molde comparing, the default hyperparameter value were used. This are provided by Parsnip R package without post-processing and hyperparameter optimzation. It is possible, although unlikely, that a different package or under different hyperparameters could result different conclusions, For calculating metrics a cross-validations with 10 folds and no repetition was used. R Core Team (2022) How to run and reproduce the conclusions Environment Info sessionInfo() ## R version 4.1.3 (2022-03-10) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19044) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Portuguese_Portugal.1252 LC_CTYPE=Portuguese_Portugal.1252 ## [3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C ## [5] LC_TIME=Portuguese_Portugal.1252 ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.26 digest_0.6.29 R6_2.5.1 jsonlite_1.8.0 ## [5] magrittr_2.0.3 evaluate_0.15 stringi_1.7.6 rlang_1.0.2 ## [9] cli_3.3.0 renv_0.15.5 rstudioapi_0.13 jquerylib_0.1.4 ## [13] bslib_0.3.1 rmarkdown_2.14 tools_4.1.3 stringr_1.4.0 ## [17] xfun_0.31 yaml_2.3.5 fastmap_1.1.0 compiler_4.1.3 ## [21] htmltools_0.5.2 knitr_1.39 sass_0.4.1 References "],["logistic-vs-knn.html", "1 logistic vs knn 1.1 Dataset definition 1.2 Model fitting 1.3 Compare results 1.4 Conclusion", " 1 logistic vs knn 1.1 Dataset definition When comparing a Logistic regression model agains a Nearest Neighbour model we are comparing a highly biased and a very flexible approach. While a logostic regression assumes a linear border between both classes, NN makes no assumptions and and relies on local information (by k neighbours) to predict a class. Given the base assumption of a linear boundary by logistic regression, a model whose border differs, substantially from a line we expect will performe badly. On the other hand the lack of linearity wont an issue for knn. Therefore, we built two datasets with 1000 observations from a Uniform Distribution, one with a classification provided by a linear model of \\(X1 &gt;= X2\\) and a narrow quadratic function \\(abs(1,2 * X_{1} - 5)^2 + 2\\). decision_fun_linear &lt;- function(x1, x2){ res &lt;- ifelse(x2 &gt;= x1, 1, 0) return(res) } decision_fun_quadratic &lt;- function(x1, x2){ res &lt;- ifelse(x2 &gt;= abs( (1.2 * x1 - 5)^2 + 2 ), 1, 0) return(res) } dataset_linear &lt;- dataset_gen_unif(class_fun = decision_fun_linear, size = 1000) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. dataset_quadratic &lt;- dataset_gen_unif(class_fun = decision_fun_quadratic, size = 1000) grid.arrange( dataset_linear$border_plot, dataset_quadratic$border_plot, nrow = 1, top = &quot;Synthetic Generated Datasets&quot;, bottom = grid::textGrob( &quot;Dashed line represent optimal bayes decision border&quot;, gp = grid::gpar(fontface = 3, fontsize = 9) ) ) 1.2 Model fitting Each dataset was divided on training and test dataset using a 80/20 split. # 0. Separate test and train data_linear &lt;- dataset_linear$dataset data_linear$g &lt;- factor(data_linear$g) split &lt;- initial_split(data_linear, prop = 0.8) train_data_linear &lt;- training(split) test_data_linear &lt;- testing(split) data_quadratic &lt;- dataset_quadratic$dataset data_quadratic$g &lt;- factor(data_quadratic$g) split &lt;- initial_split(data_quadratic, prop = 0.8) train_data_quadratic &lt;- training(split) test_data_quadratic &lt;- testing(split) 1.2.1 Logistic regression ## Create workflow ### Logistic regression ------------------------- # 1. specify the model logistic_reg_glm_spec &lt;- logistic_reg(mode = &quot;classification&quot;) %&gt;% set_engine(&#39;glm&#39;, family = &quot;binomial&quot;) # 2. preprocessing preprocess &lt;- recipe(g ~ x1 + x2 , data = train_data_linear) # 3, Buildworkflow logit_wflow &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(preprocess) # 4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) folds_linear &lt;- vfold_cv(train_data_linear, v = 10) folds_quadratic &lt;- vfold_cv(train_data_quadratic, v = 10) logit_metrics_linear &lt;- logit_wflow %&gt;% fit_resamples(folds_linear, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold02: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold03: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold04: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold05: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold06: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold07: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold08: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold09: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold10: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... logit_metrics_quadratic &lt;- logit_wflow %&gt;% fit_resamples(folds_quadratic, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; # 5. Performance metrics over the validation set logit_metrics_linear &lt;- collect_metrics(logit_metrics_linear, summarize = FALSE) logit_metrics_quadratic &lt;- collect_metrics(logit_metrics_quadratic, summarize = FALSE) # 6. Fits final model logit_linear_fit &lt;- logit_wflow %&gt;% fit(train_data_linear) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred logit_linear_model &lt;- extract_fit_parsnip(logit_linear_fit) logit_quadratic_fit &lt;- logit_wflow %&gt;% fit(train_data_quadratic) logit_quadratic_model &lt;- extract_fit_parsnip(logit_quadratic_fit) 1.2.2 KNN ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 1.3 Compare results The plots below show the resulting decision bondaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 1.4 Conclusion Given is simplicity and explicit difference between each class, both model perform very well on a dataset with a clear linear bondary. That is visible on both the training and test dataset although with an edge towards the logistic regression. It is when the boundary aliviates the linearity condition that knn really outshines the logistic output. It is important to notice that given the fact that the data derive from such strong definitions, it lakes randomness and therefore its not easy to identify signs of overfitting. "],["linear-discriminante-analysis-vs-decision-tree.html", "2 Linear Discriminante Analysis vs Decision tree 2.1 Dataset definition 2.2 Model fitting 2.3 Decision tree 2.4 Compare results 2.5 Conclusion", " 2 Linear Discriminante Analysis vs Decision tree 2.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 2.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. # 0. Separate test and train data_lda &lt;- dataset_lda$dataset data_lda$g &lt;- factor(data_lda$g) split &lt;- initial_split(data_lda, prop = 0.8) train_data_lda &lt;- training(split) test_data_lda &lt;- testing(split) data_dtree &lt;- dataset_dtree$dataset data_dtree$g &lt;- factor(data_dtree$g) split &lt;- initial_split(data_dtree, prop = 0.8) train_data_dtree &lt;- training(split) test_data_dtree &lt;- testing(split) 2.2.1 LDA Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Create workflow ### LDA regression ------------------------- # 1. specify the model discrim_linear_MASS_spec &lt;- discrim_linear() %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;MASS&quot;) # 2. preprocessing preprocess &lt;- recipe(g ~ x1 + x2 , data = train_data_lda) # 3, Buildworkflow lda_wflow &lt;- workflow() %&gt;% add_model(discrim_linear_MASS_spec) %&gt;% add_recipe(preprocess) # 4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) folds_lda &lt;- vfold_cv(train_data_lda, v = 10) folds_dtree &lt;- vfold_cv(train_data_dtree, v = 10) lda_metrics_dataset_lda &lt;- lda_wflow %&gt;% fit_resamples(folds_lda, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; lda_metrics_dataset_dtree &lt;- lda_wflow %&gt;% fit_resamples(folds_dtree, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; # 5. Performance metrics over the validation set lda_metrics_dataset_lda &lt;- collect_metrics(lda_metrics_dataset_lda, summarize = FALSE) lda_metrics_dataset_dtree &lt;- collect_metrics(lda_metrics_dataset_dtree, summarize = FALSE) # 6. Fits final model lda_ldaFit &lt;- lda_wflow %&gt;% fit(train_data_lda) lda_ldaModel &lt;- extract_fit_parsnip(lda_ldaFit) lda_dtreeFit &lt;- lda_wflow %&gt;% fit(train_data_dtree) lda_dtreeModel &lt;- extract_fit_parsnip(lda_dtreeFit) 2.3 Decision tree ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 2.4 Compare results The plots below show the resulting decision bondaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 2.5 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["linear-discriminante-analysis-vs-decision-tree-1.html", "3 Linear Discriminante Analysis vs Decision tree", " 3 Linear Discriminante Analysis vs Decision tree "],["linear-discriminante-analysis-vs-logistic-regression.html", "4 Linear Discriminante Analysis vs Logistic Regression 4.1 Dataset definition 4.2 Model fitting 4.3 Compare results 4.4 Conclusion", " 4 Linear Discriminante Analysis vs Logistic Regression 4.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 4.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold02: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold03: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold04: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold05: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold06: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold07: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold08: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold09: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## ! Fold10: preprocessor 1/1, model 1/1: glm.fit: algorithm did not converge, glm.fi... ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold02: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold03: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold04: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold05: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold06: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold07: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold08: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold09: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## ! Fold10: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0... ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred 4.3 Compare results The plots below show the resulting decision boundaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 4.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["decision-tree-vs-tree-boosting.html", "5 Decision tree vs tree boosting 5.1 Dataset definition 5.2 Model fitting 5.3 Compare results 5.4 Conclusion", " 5 Decision tree vs tree boosting 5.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 5.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 5.3 Compare results The plots below show the resulting decision boundaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 5.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["svm-radial-vs-svm-linear.html", "6 SVM Radial vs SVM linear 6.1 Dataset definition 6.2 Model fitting 6.3 Compare results 6.4 Conclusion", " 6 SVM Radial vs SVM linear 6.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 6.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## maximum number of iterations reached 0.0006068825 -0.0006062308 6.3 Compare results The plots below show the resulting decision boundaries ## Warning: stat_contour(): Zero contours were generated ## Warning in min(x): no non-missing arguments to min; returning Inf ## Warning in max(x): no non-missing arguments to max; returning -Inf The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 6.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["svm-radial-vs-svm-linear-1.html", "7 SVM Radial vs SVM linear 7.1 Dataset definition 7.2 Model fitting 7.3 Compare results 7.4 Conclusion", " 7 SVM Radial vs SVM linear 7.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 7.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## Setting default kernel parameters ## maximum number of iterations reached 0.0006068825 -0.0006062308 7.3 Compare results The plots below show the resulting decision boundaries ## Warning: stat_contour(): Zero contours were generated ## Warning in min(x): no non-missing arguments to min; returning Inf ## Warning in max(x): no non-missing arguments to max; returning -Inf The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 7.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["mlp-vs-knn.html", "8 MLP vs knn 8.1 Dataset definition 8.2 Model fitting 8.3 Compare results 8.4 Conclusion", " 8 MLP vs knn 8.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 8.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 8.3 Compare results The plots below show the resulting decision boundaries ## Warning: stat_contour(): Zero contours were generated ## Warning in min(x): no non-missing arguments to min; returning Inf ## Warning in max(x): no non-missing arguments to max; returning -Inf The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 8.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["svm-radial-vs-svm-linear-2.html", "9 SVM Radial vs SVM linear 9.1 Dataset definition 9.2 Model fitting 9.3 Compare results 9.4 Conclusion", " 9 SVM Radial vs SVM linear 9.1 Dataset definition Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 9.2 Model fitting Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. ## Warning: The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; ## The `...` are not used in this function but one or more objects were passed: &#39;verbose&#39; 9.3 Compare results The plots below show the resulting decision boundaries The plots shows the evolution of key metrics over crossvalidation trainning The plots show the metrics between different models 9.4 Conclusion Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. "],["references.html", "References", " References ## @Manual{R-knitr, ## title = {knitr: A General-Purpose Package for Dynamic ## Report Generation in R}, ## author = {Yihui Xie}, ## year = {2022}, ## note = {R package version 1.39}, ## url = {https://yihui.org/knitr/}, ## } ## ## @Manual{R-stringr, ## title = {stringr: Simple, Consistent Wrappers for Common ## String Operations}, ## author = {Hadley Wickham}, ## year = {2019}, ## note = {R package version 1.4.0}, ## url = {https://CRAN.R-project.org/package=stringr}, ## } ## ## @Book{knitr2015, ## title = {Dynamic Documents with {R} and knitr}, ## author = {Yihui Xie}, ## publisher = {Chapman and Hall/CRC}, ## address = {Boca Raton, Florida}, ## year = {2015}, ## edition = {2nd}, ## note = {ISBN 978-1498716963}, ## url = {https://yihui.org/knitr/}, ## } ## ## @InCollection{knitr2014, ## booktitle = {Implementing Reproducible Computational ## Research}, ## editor = {Victoria Stodden and Friedrich Leisch and Roger ## D. Peng}, ## title = {knitr: A Comprehensive Tool for Reproducible ## Research in {R}}, ## author = {Yihui Xie}, ## publisher = {Chapman and Hall/CRC}, ## year = {2014}, ## note = {ISBN 978-1466561595}, ## url = {http://www.crcpress.com/product/isbn/ ## 9781466561595}, ## } "],["references-1.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
