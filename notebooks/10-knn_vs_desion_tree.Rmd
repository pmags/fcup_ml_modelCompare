---
editor_options:
  chunk_output_type: console
---

# Nearest neighbour vs. decision tree


Both are non-parametric. This means that the data distribution cannot be defined in a few parameters. In other words, Decision trees and KNN’s don’t have an assumption on the distribution of the data. While decision tree supports automatic feature interaction, KNN doesn’t.Moreover decision trees can be faster, however, KNN tends to be slower with large datasets because it scans the whole dataset to predict as it doesn’t generalize the data in advance.


Decision Trees:
Advantages:

* Decision trees are effective in capturing non-linear relationships which can be difficult to achieve with other algorithms like Support Vector Machine and Linear Regression.

* Easy to explain to people: This is a great aspect of decision trees. The outputs are easy to read without requiring statistical knowledge or complex concepts.

* Some people believe decision trees more closely mirror human decision-making than others like regression and classification approaches.

* Trees can be displayed graphically and can be easily interpreted by non-experts.

* Decision trees can easily handle qualitative (categorical) features without the need to create dummy variables.

Disadvantages:

* don’t have the same level of predictive accuracy as some of other regression and classification approaches

* trees can be non-robust. Eg. small change in the data can cause a large change in the final estimated tree

* As the tree grows in size, it becomes prone to overfitting and requires pruning

KNN:
Advantages:

* Simple and intuitive: Similar to decision trees it is simple and easy to explain to laypeople.

* Non-parametric, therefore, it doesn’t have any assumptions on the data distribution

* No training step: KNN is more of an exception to the general machine learning workflow. It doesn’t have a training/validation/test set. The model created with KNN is available in a labeled dataset, placed in metric space. Say, if you want to classify any object, the model has to read through all the data and compare the distances of the closest objects.

* Easy to implement for multi-class problems: Compared to other algorithms, it is very easy to predict multiclass problems. Just supply the ‘k’ a value that is equivalent to the number of classes and you are good to go.

* Few hyperparameters: When working with K-NN, you just need to provide two parameters, k (the numbers of neighbors to consider) and the choice of Distance Function (e.g. Euclidean, Manhattan distance).

* Used for classification and regression: It can be used for classification and regression

* Instance-based learning (lazy learning): You don’t need to fit a model in advance, just provide the data point and it will give you the prediction.

Disadvantages:

* Slow with a larger dataset. If it is going to classify a new sample, it will have to read the whole dataset, hence, it becomes very slow as the dataset increases.

* Curse of dimensionality: KNN is more appropriate to use when you have a small number of inputs. If the number of variables grows, the KNN algorithm will have a hard time predicting the output of a new data point.

* Feature inputs need to be scaled: It is a must that the features should be scaled. KNN uses distance criteria, like Euclidean or Manhattan distances, therefore, it is very important that all the features have the same scale.

* Outlier sensitivity: KNN is very sensitive to outliers. Since it is an instance-based algorithm based on the distance criteria, if we have some outliers in the data, it is liable to create a biased outcome.

* Missing Value not treated: It is not capable of treating or dealing with missing values

* Class imbalance can be an issue: If we have an imbalanced class data, the algorithm might wrongly pick the majority class.
```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}
# libraries
library(tidyverse)
library(tidymodels)
library(workflowsets)
library(gridExtra)

# set global seed for reproducibility
set.seed(123)

```



For dataset of KNN we chose a dataset with a similar distribution and dispersion between the class, to avoid the confusion of the model in the bodaries areas.
For dataset for decision tree,we generate data without clear boundaries and a unbalanced number of samples between the classes, because we kwon this can affect the perform of KNN.

```{r}
# knn

l_mu_1 <- list(
  "g1" = c(2,2), 
  "g2" = c(3,6),
  "g3" = c(5,4)
  )

l_cvm_1 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )

l_w_1 <- list(
  "wg1" = 1/3, 
  "wg2" = 1/3,
  "wg3" = 1/3
  )


decision_1 <- function(x1,x2, l_mu_1, l_cvm_1){
  
l_mu_1 <- list(
  "g1" = c(2,2), 
  "g2" = c(3,6),
  "g3" = c(5,4)
  )

l_cvm_1 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu_1[[1]], sigma = l_cvm_1[[1]]) * 1/3 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu_1[[2]], sigma = l_cvm_1[[2]]) * 1/3
  px_2 <- dmvnorm(c(x1,x2), mean = l_mu_1[[3]], sigma = l_cvm_1[[3]]) * 1/3
  
  g <- case_when(
    
    px_0 > px_1 & px_0 > px_2 ~ 0,
    px_1 > px_0 & px_1 > px_2 ~ 1,
    px_2 > px_0 & px_2 > px_1 ~ 2,
    TRUE ~ 2
  )
  
  return(g)
}

dataset_knn <- dataset_gen_mvnorm(l_mu_1, l_cvm_1, l_w_1, class_fun = decision_1, n_g = 3)


```

```{r}

l_mu_2 <- list(
    "g1" = c(1,2), 
    "g2" = c(2,2),
    "g3" = c(3,4)
  )

l_cvm_2 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )
  
l_w_2 <- list(
  "wg1" = 0.3, 
  "wg2" = 0.5,
  "wg3" = 0.2
  )


decision_2 <- function(x1,x2, l_mu_2, l_cvm_2,l_w_2, size=1000){
  
l_mu_2 <- list(
    "g1" = c(1,2), 
    "g2" = c(2,2),
    "g3" = c(3,4)
  )

l_cvm_2 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )
l_w_2 <- list(
  "wg1" = 0.3, 
  "wg2" = 0.5,
  "wg3" = 0.2
  )

  px_0 <- dmvnorm(c(x1,x2), mean = l_mu_2[[1]], sigma = l_cvm_2[[1]])
  px_1 <- dmvnorm(c(x1,x2),mean = l_mu_2[[2]], sigma = l_cvm_2[[2]])
  px_2 <- dmvnorm(c(x1,x2), mean = l_mu_2[[3]], sigma = l_cvm_2[[3]])
  
  g <- case_when(
    
    px_0 > px_1 & px_0 > px_2 ~ 0,
    px_1 > px_0 & px_1 > px_2 ~ 1,
    px_2 > px_0 & px_2 > px_1 ~ 2,
    TRUE ~ 2
  )
  
  return(g)
}

dataset_DT <- dataset_gen_mvnorm(l_mu_2, l_cvm_2, l_w_2, class_fun = decision_2, n_g = 3)

```

```{r}

grid.arrange(
  dataset_knn$border_plot, 
  dataset_DT$border_plot, 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting
 

```{r}
# define workflows

### knn

# 1. specify the model

nearest_neighbor_kknn_spec <-
  nearest_neighbor(neighbors = 3) %>%
  set_engine('kknn') %>%
  set_mode('classification')




# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_knn$dataset)
  
# 3, Buildworkflow
nearest_neighbor_kknn_wflow <- 
  workflow() %>% 
  add_model(nearest_neighbor_kknn_spec) %>% 
  add_recipe(preprocess)


### DT

# 1. specify the model
decision_tree_rpart_spec <-
  decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')



# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_DT$dataset)
  
# 3, Buildworkflow
decision_tree_rpart_wflow <- 
  workflow() %>% 
  add_model(decision_tree_rpart_spec) %>% 
  add_recipe(preprocess)

```


```{r}

data <- list(data_knn=dataset_knn, data_DT=dataset_DT)
workflows <- list(knn=nearest_neighbor_kknn_wflow , DT = decision_tree_rpart_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## knn

metrics_ds1_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$knn)

metrics_ds2_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$knn)


## DT

metrics_ds1_DT <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$DT)

metrics_ds2_DT <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$DT)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_DT <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$DT, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_DT <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$DT, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```

The plots below show the resulting decision boundaries

```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$knn + labs(title = "dataset 1 - knn"),
  compare_fit$plots$model_decision$dataset1$DT + labs(title = "dataset 1 - DT"),
  compare_fit$plots$model_decision$dataset2$knn + labs(title = "dataset 2 - knn"),
  compare_fit$plots$model_decision$dataset2$DT + labs(title = "dataset 2 - DT"),
  nrow = 2,
  top = "Aplying a svm_linearistic model to both datasets",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border. Visually is possible to conclude that the svm_linearistic model fits better a linear  frontier while struggling when facing a curved frontier",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```
In the above graphs

The plots shows the evolution of key metrics over crossvalidation trainning


```{r}

grid.arrange(
  train_metrics_ds1_knn + labs(title = "dataset 1 - knn"), 
  train_metrics_ds1_DT + labs(title = "dataset 1 - DT"), 
  train_metrics_ds2_knn + labs(title = "dataset 2 - knn"),
  train_metrics_ds2_DT + labs(title = "dataset 2 - DT"), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_knn$cf_plot ,
  metrics_ds1_DT$cf_plot ,
  metrics_ds2_knn$cf_plot ,
  metrics_ds2_DT$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_knn$roc_curve ,
  metrics_ds1_DT$roc_curve ,
  metrics_ds2_knn$roc_curve ,
  metrics_ds2_DT$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

  metrics_ds1_knn$auc_roc
  metrics_ds1_DT$auc_roc
  metrics_ds2_knn$auc_roc
  metrics_ds2_DT$auc_roc

```

## Conclusion
Decision tree is used to partition the data to find accurate result but in KNN it used to find similar values from the data. Each and every algorithm has its own merits and demerits and never ever all algorithms have satisfied all criteria and requirements. Each algorithm has its own specification, so algorithms should be chosen according to the requirements. 

