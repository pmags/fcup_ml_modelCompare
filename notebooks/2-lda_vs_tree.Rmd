---
editor_options:
  chunk_output_type: console
---

# Linear Discriminante Analysis vs Decision tree


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)
library(discrim)


# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 


```{r}

l_mu <- list(
  "g1" = c(5,3), 
  "g2" = c(3,5)
  )

l_cvm <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2)
  )

l_w <- list(
  "wg1" = 0.5, 
  "wg2" = 0.5
  )


decision <- function(x1,x2, l_mu, l_cvm){
  
  l_mu <- list(
    "g1" = c(5,3), 
    "g2" = c(3,5)
  )
  
  l_cvm <- list( 
    "covg1" = matrix(c(1,0,0,1),2,2),
    "covg2" = matrix(c(1,-0.4,-0.4,1),2,2)
  )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu[[1]], sigma = l_cvm[[1]]) * 0.5 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu[[2]], sigma = l_cvm[[2]]) * 0.5
  
  
  g <- case_when(
    
    px_0 > px_1 ~ 0,
    TRUE ~ 1
  )
  
  return(g)
}

dataset_lda2 <- dataset_gen_mvnorm(l_mu, l_cvm, l_w, class_fun = decision)

decision_fun_linear <- function(x1, x2){
  res <- ifelse(x2 >= x1, 1, 0)
  return(res)
}

dataset_lda <- dataset_gen_unif(class_fun = decision_fun_linear, size = 1000)

decision_fun_normal <- function(x1, x2){
  
  mu <- c(6,6)
  cvar <- matrix(c(0.5,0,0,0.5), 2, 2)
  p <- mvtnorm::pmvnorm(c(x1,x2), mean = mu, sigma = cvar)
  
  res <- ifelse(p[1] <= 0.5, 1, 0)
  return(res)
}

dataset_dtree <- dataset_gen_unif(class_fun = decision_fun_normal)


grid.arrange(
  dataset_dtree$border_plot, 
  dataset_lda$border_plot, 
  nrow = 1,
  top = "Optimal decision border")

```

## Model fitting

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 

```{r echo=TRUE}

# 0. Separate test and train

data_lda <- dataset_lda$dataset
data_lda$g <- factor(data_lda$g)

split <- initial_split(data_lda, prop = 0.8)

train_data_lda <- training(split)
test_data_lda <- testing(split)




data_dtree <- dataset_dtree$dataset
data_dtree$g <- factor(data_dtree$g)

split <- initial_split(data_dtree, prop = 0.8)

train_data_dtree <- training(split)
test_data_dtree <- testing(split)

```


### LDA

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 

```{r echo=TRUE}

## Create workflow

### LDA regression -------------------------


# 1. specify the model
discrim_linear_MASS_spec <-
  discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = train_data_lda)
  
# 3, Buildworkflow
lda_wflow <- 
  workflow() %>% 
  add_model(discrim_linear_MASS_spec) %>% 
  add_recipe(preprocess)

# 4. Fit model
fit_control <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

folds_lda <- vfold_cv(train_data_lda, v = 10)
folds_dtree <- vfold_cv(train_data_dtree, v = 10)


lda_metrics_dataset_lda <- 
  lda_wflow %>% 
  fit_resamples(folds_lda, verbose = TRUE, control = fit_control)

lda_metrics_dataset_dtree <- 
  lda_wflow %>% 
  fit_resamples(folds_dtree, verbose = TRUE, control = fit_control)

# 5. Performance metrics over the validation set
lda_metrics_dataset_lda <- collect_metrics(lda_metrics_dataset_lda, summarize = FALSE)
lda_metrics_dataset_dtree <- collect_metrics(lda_metrics_dataset_dtree, summarize = FALSE)


# 6. Fits final model
lda_ldaFit <- 
  lda_wflow %>% 
  fit(train_data_lda)

lda_ldaModel <- extract_fit_parsnip(lda_ldaFit)

lda_dtreeFit <- 
  lda_wflow %>% 
  fit(train_data_dtree)

lda_dtreeModel <- extract_fit_parsnip(lda_dtreeFit)

```


## Decision tree


```{r}

### Decision tree --------------- -------------------------

# 1. specify the model
decision_tree_rpart_spec <-
  decision_tree() %>% 
  set_engine("rpart") %>%
  set_mode("classification")

# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = train_data_lda) # just to set the relation, irrelevante which dataset used
  
# 3, Buildworkflow
dtree_wflow <- 
  workflow() %>% 
  add_model(decision_tree_rpart_spec) %>% 
  add_recipe(preprocess)

# 4. Fit model

dtree_metrics_dataset_lda <- 
  dtree_wflow %>% 
  fit_resamples(folds_lda, verbose = TRUE, control = fit_control) # folds and fit control defined above

dtree_metrics_dataset_dtree <- 
  dtree_wflow %>% 
  fit_resamples(folds_dtree, verbose = TRUE, control = fit_control)

# 5. Performance metrics over the validation set
dtree_metrics_dataset_lda <- collect_metrics(dtree_metrics_dataset_lda, summarize = FALSE)
dtree_metrics_dataset_dtree <- collect_metrics(dtree_metrics_dataset_dtree, summarize = FALSE)


# 6. Fits final model
dtree_fit_dataset_lda <- 
  dtree_wflow %>% 
  fit(train_data_lda)

dtree_model_dataset_lda <- extract_fit_parsnip(dtree_fit_dataset_lda)

dtree_fit_dataset_dtree <- 
  dtree_wflow %>% 
  fit(train_data_dtree)

dtree_model_dataset_dtree <- extract_fit_parsnip(dtree_fit_dataset_dtree)


```


## Compare results

```{r}

# lda model

# fit model to grid to find border linear

lda_grid_lda_dataset <- 
  dataset_lda$cond %>% 
  bind_cols(
    predict(lda_ldaModel, new_data = dataset_lda$cond),
    predict(lda_ldaModel, new_data = dataset_lda$cond, type = "prob"),
  ) %>% 
  mutate(
    .pred_1 = round(.pred_1, 3),
    .pred_0 = round(.pred_0, 3),
    decision = .pred_1 - .pred_0
  )

plot_lda_decision_lda_dataset  <- dataset_lda$border_plot +
   geom_contour(data = lda_grid_lda_dataset , aes(x = x1,y = x2, z = decision), color = "Purple", breaks = 0, size = 1, alpha = 0.5) +
  geom_point(data = lda_grid_lda_dataset, aes(x = x1, y = x2, color =.pred_class ), size = 0.1, alpha = 0.1)


# fit model to grid to find border quadratic

lda_grid_dtree_dataset <- 
  dataset_dtree$cond %>% 
  bind_cols(
    predict(lda_ldaModel, new_data = dataset_dtree$cond),
    predict(lda_ldaModel, new_data = dataset_dtree$cond, type = "prob"),
  ) %>% 
  mutate(
    .pred_1 = round(.pred_1, 3),
    .pred_0 = round(.pred_0, 3),
    decision = .pred_1 - .pred_0
  )

plot_lda_decision_dtree_dataset <- dataset_dtree$border_plot +
   geom_contour(data = lda_grid_dtree_dataset , aes(x = x1,y = x2, z = decision), color = "Purple", breaks = 0, size = 1, alpha = 0.5) +
  geom_point(data = lda_grid_dtree_dataset , aes(x = x1, y = x2, color =.pred_class ), size = 0.1, alpha = 0.1)


```



```{r}

# Decision tree

# fit model to grid to find border linear

dtree_grid_lda_dataset <- 
  dataset_lda$cond %>% 
  bind_cols(
    predict(dtree_model_dataset_lda, new_data = dataset_lda$cond),
    predict(dtree_model_dataset_lda, new_data = dataset_lda$cond, type = "prob"),
  ) %>% 
  mutate(
    .pred_1 = round(.pred_1, 3),
    .pred_0 = round(.pred_0, 3),
    decision = .pred_1 - .pred_0
  )

plot_dtree_decision_lda_dataset <- dataset_lda$border_plot +
   geom_contour(data = dtree_grid_lda_dataset, aes(x = x1,y = x2, z = decision), color = "Purple", breaks = 0, size = 1, alpha = 0.5) +
  geom_point(data = dtree_grid_lda_dataset, aes(x = x1, y = x2, color =.pred_class ), size = 0.1, alpha = 0.1)


# fit model to grid to find border quadratic

dtree_grid_dtree_dataset <- 
  dataset_dtree$cond %>% 
  bind_cols(
    predict(dtree_model_dataset_dtree, new_data = dataset_dtree$cond),
    predict(dtree_model_dataset_dtree, new_data = dataset_dtree$cond, type = "prob"),
  ) %>% 
  mutate(
    .pred_1 = round(.pred_1, 3),
    .pred_0 = round(.pred_0, 3),
    decision = .pred_1 - .pred_0
  )

plot_dtree_decision_dtree_dataset <- dataset_dtree$border_plot +
   geom_contour(data = dtree_grid_dtree_dataset, aes(x = x1,y = x2, z = decision), color = "Purple", breaks = 0, size = 1, alpha = 0.5) +
  geom_point(data = dtree_grid_dtree_dataset, aes(x = x1, y = x2, color =.pred_class ), size = 0.1, alpha = 0.1)


```



```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

lda_metrics_lda_dataset <- 
  model_metrics(test_data = test_data_lda, 
                model = lda_ldaModel)

lda_metrics_dtree_dataset <- 
  model_metrics(test_data = test_data_dtree, 
                model = lda_dtreeModel)

## knn model

dtree_metrics_lda_dataset <- 
  model_metrics(test_data = test_data_lda, 
                model = dtree_model_dataset_lda)

dtree_metrics_dtree_dataset <- 
  model_metrics(test_data = test_data_dtree, 
                model = dtree_model_dataset_dtree)

```



```{r echo=FALSE}

# logistic regression metrics during training

lda_ldaDataset_training_plot <- 
  ggplot(lda_metrics_dataset_lda, aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

lda_dtreeDataset_training_plot <- 
  ggplot(lda_metrics_dataset_dtree, aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training


dtree_ldaDataset_training_plot <- 
  ggplot(dtree_metrics_dataset_lda, aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

dtree_dtreeDataset_training_plot <- 
  ggplot(dtree_metrics_dataset_dtree, aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


```


The plots below show the resulting decision bondaries

```{r}

## compare results

grid.arrange(
  plot_lda_decision_lda_dataset,
  plot_lda_decision_dtree_dataset,
  plot_dtree_decision_lda_dataset ,
  plot_dtree_decision_dtree_dataset,
  nrow = 2,
  top = "Aplying a logistic model to both datasets",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border. Visually is possible to conclude that the logistic model fits better a linear  frontier while struggling when facing a curved frontier",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```


The plots shows the evolution of key metrics over crossvalidation trainning


```{r}

grid.arrange(
  lda_ldaDataset_training_plot, 
  lda_dtreeDataset_training_plot, 
  dtree_ldaDataset_training_plot,
  dtree_dtreeDataset_training_plot,
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```



The plots show the metrics between different models

```{r}

grid.arrange(
  lda_metrics_lda_dataset$cf_plot + labs(title = paste0("Lda acc: ",lda_metrics_lda_dataset$acc$.estimate)) ,
  lda_metrics_dtree_dataset$cf_plot + labs(title =  paste0("dtree acc: ",lda_metrics_dtree_dataset$acc$.estimate)),
  dtree_metrics_lda_dataset$cf_plot + labs(title = paste0("Lda acc: ", dtree_metrics_lda_dataset$acc$.estimate)),
  dtree_metrics_dtree_dataset$cf_plot + labs(title= paste0("dtree acc: ",dtree_metrics_dtree_dataset$acc$.estimate ) ),
  nrow = 2,
  top = "Confusion Matrix"
)


```



```{r}

grid.arrange(
  lda_metrics_lda_dataset$roc_curve + labs(title = "lda"),
  lda_metrics_dtree_dataset$roc_curve + labs(title = "dtree"),
  dtree_metrics_lda_dataset$roc_curve + labs(title = "lda"),
  dtree_metrics_dtree_dataset$roc_curve + labs(title= "dtree"),
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 