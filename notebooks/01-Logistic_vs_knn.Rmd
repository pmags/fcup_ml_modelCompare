---
editor_options:
  chunk_output_type: console
---

# Logistic regression vs Nearest Neighbour

```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(tidymodels)
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

Logistic regressions and Nearest Neighbor methods are oposing extremes of the Bias-Variance scale. A logistic regression estimates $Pr(G = 1 | X)$ using the *Logistic function* $p(X) = \frac{e^{\beta_{0} + \beta_{1}X}}{1+\beta_{0} + \beta_{1}X }$. This translates to a linear classification boundary when ploting with independent variables.

On the other hand, Nearest Neighbor makes no assumptions and relies solely on local (neighbor) information to predict the $Pr(G = 1 | X)$. This approach makes for a very flexible model but highly sensitive to newer information.

Given each approach oposing characteristics, our initial hypothesis is that, given a dataset with a perfectly linear decision boundary, will be a easily estimated by the biased Logistic regression but the NN approach, given its nature, will struggle since points close to the border will have a strong influence. Therefore, we defined the comparing datasets as follows:

1. A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs above $X_{1} - X_{2} = 0$ are classified as **1** and **0** if below;

2. A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs above $abs(1,2X_{1} - 5)^2 + 2 - X_{2} = 0$ are classified as **1** and **0** if below;

```{r echo=TRUE}

decision_fun_linear <- function(x1, x2){
  res <- ifelse(x2 >= x1, 1, 0)
  return(res)
}

decision_fun_quadratic <- function(x1, x2){
  res <- ifelse(x2 >= abs( (1.2 * x1 - 5)^2 + 2 ), 1, 0)
  return(res)
}

dataset_linear <- dataset_gen_unif(class_fun = decision_fun_linear, size = 1000)
dataset_quadratic <- dataset_gen_unif(class_fun = decision_fun_quadratic, size = 1000)

grid.arrange(
  dataset_linear$border_plot + labs(subtitle ="Linear decision"  ), 
  dataset_quadratic$border_plot + labs(subtitle = "Quadratic decision"), 
  nrow = 1,
  top = "Synthetic Generated Datasets",
  bottom = grid::textGrob(
    "Dashed line represent optimal bayes decision boundary",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```


## Model fitting

The following workflow (using @R-tidymodels) was executed in order to fit and evaluate each model given the above defined datasets:

1. Train - Test split by 80% Train - 20% Test,

2. Define 10 random folds splitting Train and Validation,

3. Fit models to each fold,

4. Calculate Fold metrics,

5. Fit to all train data and extract model,

6. Create plots with estimated decision boundaries


```{r}
# define workflows

### Logistic regression

# 1. specify the model
logistic_reg_glm_spec <-
  parsnip::logistic_reg(mode = "classification") %>%
  parsnip::set_engine('glm', family = "binomial") 


# 2. preprocessing 
preprocess <- 
  recipes::recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  

# 3, Buildworkflow
logit_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(logistic_reg_glm_spec) %>% 
  workflows::add_recipe(preprocess)


### SVM radial

# 1. specify the model
nearest_neighbor_kknn_spec <-
  parsnip::nearest_neighbor() %>%
  parsnip::set_engine('kknn') %>% # Defaultk = 5
  parsnip::set_mode('classification')

# 2. preprocessing 
preprocess <- 
  recipes::recipe(g ~ x1 + x2 , data = dataset_linear$dataset) # just to set the relation, irrelevant which dataset used
  
# 3, Buildworkflow
knn_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(nearest_neighbor_kknn_spec) %>% 
  workflows::add_recipe(preprocess)

```


```{r}

data <- list(dataset_linear, dataset_quadratic)
workflows <- list(logistic = logit_wflow, knn = knn_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r message=TRUE, warning=TRUE, include=FALSE}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_logistic <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$logistic)

metrics_ds2_logistic <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$logistic)


## knn model

metrics_ds1_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$knn)

metrics_ds2_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$knn)

```


```{r message=TRUE, warning=TRUE, include=FALSE}

# logistic regression metrics during training

train_metrics_ds1_logistic <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$logistic, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_logistic <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$logistic, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```


```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$logistic + labs(subtitle = "Linear decision"),
  compare_fit$plots$model_decision$dataset1$knn + labs(subtitle = "Quadratic decision"),
  compare_fit$plots$model_decision$dataset2$logistic ,
  compare_fit$plots$model_decision$dataset2$knn,
  nrow = 2,
  top = "Logistic vs Knn (k = 3)",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border.",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```

From the above plots we can conclude that the results are much in line with our initial hypothesis. Given a linear boundary, the logistic regression was able to match to perfection while the knn with k = 3 was influenced by observations closer to the border. On the other hand, the logistic regression was unable to classify when a border was a quadratic function.

The following plot showcases the performance of each model calculated on each of the 10 validation folds. It is clear the struggle of the logistic model on a quadratic boundary.

```{r echo=FALSE}

grid.arrange(
  train_metrics_ds1_logistic + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds1_knn+ theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds2_logistic+ theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)),
  train_metrics_ds2_knn+ theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```

Below the results on the test set which confirm our preliminary conclusions

```{r}

grid.arrange(
  metrics_ds1_logistic$cf_plot ,
  metrics_ds1_knn$cf_plot ,
  metrics_ds2_logistic$cf_plot ,
  metrics_ds2_knn$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_logistic$roc_curve ,
  metrics_ds1_knn$roc_curve ,
  metrics_ds2_logistic$roc_curve ,
  metrics_ds2_knn$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

Given is simplicity and explicit difference between each class, both model perform very well on a dataset with a clear linear bondary. That is visible on both the training and test dataset although with an edge towards the logistic regression. 

