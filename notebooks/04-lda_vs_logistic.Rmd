---
editor_options:
  chunk_output_type: console
---

# Linear Discriminante Analysis vs Logistic Regression


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

On most occasions we can expect LDA and Logistic to have similar performances. Nonetheless, the underlying assumption that each feature follows a normal distribution, makes LDA more sensitive to outliers specialy on a context ot smaller number of observations.

> LDA classification rule depends on the mean of *all* of the obsevations within each class, as well as the within-class covariance matrix computed using all of the observations. In contrast, logistic regression, unlike LDA, has very low sensitivity to observations far from the decision boundary. (@James2013) 

To demonstrate our hypothesis we defined two datasets as follows:

1. A dataset of 1000 pair of (x1,x2) observations from 2 multivariable normal distribution each representing a class. The dataset is perfectly balanced;


2. A dataset of 50 pair of (x1,x2) observations from 2 multivariable normal distribution each representing a class. The dataset balance is 70/30. The most extreme value was multiplied by a factor in order to increase his impact over the mean.


```{r}

## A dataset with a multivariable normal distribution and similar covariance matrix

l_mu <- list(
  "g1" = c(6,2), 
  "g2" = c(2,6)
  )

l_cvm <- list( 
  "covg1" = matrix(c(2,0.4,0.4,2),2,2),
  "covg2" = matrix(c(2,-0.4,-0.4,2),2,2)
  )

l_w <- list(
  "wg1" = 0.5, 
  "wg2" = 0.5
  )


decision <- function(x1,x2){
  
  l_mu <- list(
    "g1" = c(6,2), 
    "g2" = c(2,6)
    )
  
  l_cvm <- list( 
    "covg1" = matrix(c(2,0.4,0.4,2),2,2),
    "covg2" = matrix(c(2,-0.4,-0.4,2),2,2)
    )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu[[1]], sigma = l_cvm[[1]]) * 0.5 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu[[2]], sigma = l_cvm[[2]]) * 0.5
  
  
  g <- case_when(
    
    px_0 > px_1 ~ 0,
    TRUE ~ 1
  )
  
  return(g)
}

dataset_mv <- dataset_gen_mvnorm(
  l_mu = l_mu, 
  l_cvm = l_cvm, 
  l_w = l_w, 
  class_fun = decision)


## A dataset with a multivariable normal distribution and similar covariance matrix

l_mu <- list(
  "g1" = c(6,3), 
  "g2" = c(3,6)
  )

l_cvm <- list( 
  "covg1" = matrix(c(3,0.6,0.6,3),2,2),
  "covg2" = matrix(c(3,-0.6,-0.6,3),2,2)
  )

l_w <- list(
  "wg1" = 0.3, 
  "wg2" = 0.7
  )


decision <- function(x1,x2){
  
  l_mu <- list(
    "g1" = c(6,3), 
    "g2" = c(3,6)
    )
  
  l_cvm <- list( 
    "covg1" = matrix(c(3,0.6,0.6,3),2,2),
    "covg2" = matrix(c(3,-0.6,-0.6,3),2,2)
    )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu[[1]], sigma = l_cvm[[1]]) * 0.5 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu[[2]], sigma = l_cvm[[2]]) * 0.5
  
  
  g <- case_when(
    
    px_0 > px_1 ~ 0,
    TRUE ~ 1
  )
  
  return(g)
}

dataset_mv_s_outlier <- dataset_gen_mvnorm(
  l_mu = l_mu, 
  l_cvm = l_cvm, 
  l_w = l_w, 
  class_fun = decision,
  outlier_boost = 2,
  size = 50)


grid.arrange(
  dataset_mv$border_plot + labs(subtitle ="1.000 obs", color = ""), 
  dataset_mv_s_outlier$border_plot+ labs(subtitle ="50 obs + outlier", color = ""), 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting

The following workflow (using @R-tidymodels) was executed in order to fit and evaluate each model given the above defined datasets:

1. Train - Test split by 80% Train - 20% Test,

2. Define 10 random folds splitting Train and Validation,

3. Fit models to each fold,

4. Calculate Fold metrics,

5. Fit to all train data and extract model,

6. Create plots with estimated decision boundaries 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# define workflows

### LDA

# 1. specify the model
discrim_linear_MASS_spec <-
  discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_mv$dataset)
  
# 3, Buildworkflow
lda_wflow <- 
  workflow() %>% 
  add_model(discrim_linear_MASS_spec) %>% 
  add_recipe(preprocess)


### Logistic

# 1. specify the model
logistic_reg_glm_spec <-
  logistic_reg(mode = "classification") %>%
  set_engine('glm', family = "binomial") 


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_mv$dataset)
  
# 3, Buildworkflow
log_wflow <- 
  workflow() %>% 
  add_model(logistic_reg_glm_spec) %>% 
  add_recipe(preprocess)

```


```{r echo=TRUE, message=FALSE, warning=FALSE}

data <- list(dataset_mv, dataset_mv_s_outlier)
workflows <- list(log = log_wflow, lda = lda_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```

## Compare results

```{r message=TRUE, warning=TRUE, include=FALSE}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_log <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$log)

metrics_ds2_log <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$log)

## lda model

metrics_ds1_lda <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$lda)

metrics_ds2_lda <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$lda)

```


```{r message=TRUE, warning=TRUE, include=FALSE}

# logistic regression metrics during training

train_metrics_ds1_log <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$log, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_log <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$log, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# lda  performance during training

train_metrics_ds1_lda <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$lda, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_lda <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$lda, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```


```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$log + labs(subtitle = "Logistic"),
  compare_fit$plots$model_decision$dataset1$lda + labs(subtitle = "Linear Disriminant"),
  compare_fit$plots$model_decision$dataset2$log,
  compare_fit$plots$model_decision$dataset2$lda,
  nrow = 2,
  top = "Logistic vs LDA",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border.",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```

Despite still being able to fit a model, as expected, the pull from the outliers has a higher effect on LDA when compared to Logistic.

```{r}

grid.arrange(
  train_metrics_ds1_log + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds1_lda + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds2_log + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)),
  train_metrics_ds2_lda + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over mv dataset and left plots reflect a dataset with outliers",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


```{r}

grid.arrange(
  metrics_ds1_log$cf_plot ,
  metrics_ds1_lda$cf_plot ,
  metrics_ds2_log$cf_plot ,
  metrics_ds2_lda$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_log$roc_curve ,
  metrics_ds1_lda$roc_curve ,
  metrics_ds2_log$roc_curve ,
  metrics_ds2_lda$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

Despite its similarities we demonstrated that under certain conditions the logistic regression can outperform the LDA approach. Because of its higher sensibility to outlier on a scenario of a small dataset with outliers, they will have a bigger impact on the classification border. 

```{r}

knitr::kable(
  
  data.frame(
    ds2_log = metrics_ds2_log$auc_roc$.estimate,
    ds2_lad = metrics_ds2_lda$auc_roc$.estimate,
    ds1_log = metrics_ds1_log$auc_roc$.estimate,
    ds1_lad = metrics_ds1_lda$auc_roc$.estimate
    )
  
)

```

