---
editor_options:
  chunk_output_type: console
---

# MLP vs knn


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  
The Nearest-Neighbor method (K-NN): The K-NN method is a non-parametric statistical pattern recognition procedure and among the various non-parametric techniques is the most intuitive, but nevertheless possesses powerful statistical properties (Toth et al., 2000).

ANN methodology: There are many ways of using ANNs in the context of RR models (Anctil et al., 2004) like network topology, training algorithm, input selection and network size optimization and each of them has a priori experience-based assumptions. Neural Networks distribute computations to processing units called neurons, grouped in layers. Three different layer types can be distinguished: input layer, connecting the input information to the network, one or more hidden layer and acting as intermediate computational layers between input and output and output layer, producing the final output (Toth et al., 2000). 


The MLP dataset is basedon a normal distibution but without clearly separated boundaries, so we expected that MLP model works well in this data.
On the other hand the knn dataset have a circular perfect boundary, so we expect that knn will be better in this dataset, even the diferences in this case is lower, because the MLP leads also weel with this boundary because can be seen as a linear boundary.
```{r}

## A pure linear boundary


l_mu_2 <- list(
    "g1" = c(1,2), 
    "g2" = c(2,2),
    "g3" = c(3,4)
  )

l_cvm_2 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )
  
l_w_2 <- list(
  "wg1" = 0.3, 
  "wg2" = 0.5,
  "wg3" = 0.2
  )


decision_2 <- function(x1,x2, l_mu_2, l_cvm_2,l_w_2, size=1000){
  
l_mu_2 <- list(
    "g1" = c(1,2), 
    "g2" = c(2,2),
    "g3" = c(3,4)
  )

l_cvm_2 <- list( 
  "covg1" = matrix(c(1,0,0,1),2,2),
  "covg2" = matrix(c(1,-0.4,-0.4,1),2,2),
  "covg3" = matrix(c(1,0.2,0.2,1),2,2)
  )
l_w_2 <- list(
  "wg1" = 0.3, 
  "wg2" = 0.5,
  "wg3" = 0.2
  )

  px_0 <- dmvnorm(c(x1,x2), mean = l_mu_2[[1]], sigma = l_cvm_2[[1]])
  px_1 <- dmvnorm(c(x1,x2),mean = l_mu_2[[2]], sigma = l_cvm_2[[2]])
  px_2 <- dmvnorm(c(x1,x2), mean = l_mu_2[[3]], sigma = l_cvm_2[[3]])
  
  g <- case_when(
    
    px_0 > px_1 & px_0 > px_2 ~ 0,
    px_1 > px_0 & px_1 > px_2 ~ 1,
    px_2 > px_0 & px_2 > px_1 ~ 2,
    TRUE ~ 2
  )
  
  return(g)
}

dataset_MLP<- dataset_gen_mvnorm(l_mu_2, l_cvm_2, l_w_2, class_fun = decision_2, n_g = 3)

## A circular boundary

decision <- function(x1, x2){
  res <- ifelse((-5 + x1)^2 + (5 - x2)^2 > 4, 1, 0)
  return(res)
}

dataset_knn <- dataset_gen_unif(
  class_fun = decision, 
  size = 1000)


grid.arrange(
  dataset_MLP$border_plot, 
  dataset_knn$border_plot, 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting


```{r}
# define workflows

### MLP model

# 1. specify the model

mlp_nnet_spec <-
  mlp() %>%
  set_engine('nnet',learn_rate=0.001) %>%
  set_mode('classification')

# 2. preprocessing 
preprocess <- 
    recipe(g ~ x1 + x2 , data = dataset_MLP$dataset)
  
# 3, Buildworkflow
mlp_nnet_wflow <- 
  workflow() %>% 
  add_model(mlp_nnet_spec) %>% 
  add_recipe(preprocess)


### knn

# 1. specify the model

nearest_neighbor_kknn_spec <-
  nearest_neighbor() %>%
  set_engine('kknn',nearest_neighbor=3) %>%
  set_mode('classification')




# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_knn$dataset)
  
# 3, Buildworkflow
nearest_neighbor_kknn_wflow <- 
  workflow() %>% 
  add_model(nearest_neighbor_kknn_spec) %>% 
  add_recipe(preprocess)



```


```{r}

data <- list(dataset_MLP, dataset_knn)
workflows <- list(MLP = mlp_nnet_wflow, knn = nearest_neighbor_kknn_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_MLP <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$MLP)

metrics_ds2_MLP <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$MLP)


## knn model

metrics_ds1_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$knn)

metrics_ds2_knn <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$knn)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_MLP <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$MLP, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_MLP <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$MLP, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_knn <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$knn, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```

The plots below show the resulting decision boundaries

```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$MLP + labs(title = "dataset 1 - MLP"),
  compare_fit$plots$model_decision$dataset1$knn + labs(title = "dataset 1 - knn"),
  compare_fit$plots$model_decision$dataset2$MLP + labs(title = "dataset 2 - MLP"),
  compare_fit$plots$model_decision$dataset2$knn + labs(title = "dataset 2 - knn"),
  nrow = 2,
  top = "Aplying a MLPistic model to both datasets",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border. Visually is possible to conclude that the MLPistic model fits better a linear  frontier while struggling when facing a curved frontier",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```

The plots shows the evolution of key metrics over crossvalidation trainning


```{r}

grid.arrange(
  train_metrics_ds1_MLP, 
  train_metrics_ds1_knn, 
  train_metrics_ds2_MLP,
  train_metrics_ds2_knn, 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_MLP$cf_plot ,
  metrics_ds1_knn$cf_plot ,
  metrics_ds2_MLP$cf_plot ,
  metrics_ds2_knn$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_MLP$roc_curve ,
  metrics_ds1_knn$roc_curve ,
  metrics_ds2_MLP$roc_curve ,
  metrics_ds2_knn$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

As we so in the above plots, the results are in line with our expectations, and we have better performance in MLP with dataset 1 (MLP dataset), a a bit better in knn with dataset 2 (knn dataset), but as refered the diference are not huge, so if we tune the parameters we can achieve a good performance with MLP for this dataset as well.