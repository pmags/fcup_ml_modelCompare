---
editor_options:
  chunk_output_type: console
---

# Decision tree vs tree boosting


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 


```{r}

## A pure linear boundary

decision <- function(x1, x2){
  res <- ifelse(x2 >= x1, 1, 0)
  return(res)
}

dataset_linear <- dataset_gen_unif(
  class_fun = decision, 
  size = 1000)

## A dataset with a multivariable normal distribution and similar covariance matrix

l_mu <- list(
  "g1" = c(6,3), 
  "g2" = c(3,6),
  "g3" = c(4,4)
  )

l_cvm <- list( 
  "covg1" = matrix(c(1,0.4,0.4,1),2,2),
  "covg2" = matrix(c(1,0,0,1),2,2),
  "covg3" = matrix(c(1,-0.4,-0.4,1),2,2)
  )

l_w <- list(
  "wg1" = 1/3, 
  "wg2" = 1/3,
  "wg3" = 1/3
  )


decision <- function(x1,x2){
  
  l_mu <- list(
    "g1" = c(6,3), 
    "g2" = c(3,6),
    "g3" = c(4,4)
    )
  
  l_cvm <- list( 
    "covg1" = matrix(c(1,0.4,0.4,1),2,2),
    "covg2" = matrix(c(1,0,0,1),2,2),
    "covg3" = matrix(c(1,-0.4,-0.4,1),2,2)
    )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu[[1]], sigma = l_cvm[[1]]) 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu[[2]], sigma = l_cvm[[2]]) 
  px_2 <- dmvnorm(c(x1,x2), mean = l_mu[[3]], sigma = l_cvm[[3]])
  
  
  g <- case_when(
    
    px_0 > px_1 & px_0 > px_2 ~ 0,
    px_1 > px_0 & px_1 > px_2 ~ 1,
    px_2 > px_0 & px_2 > px_1 ~ 2,
    TRUE ~ 2
  )
  
  return(g)
}

dataset_normal_dist <- dataset_gen_mvnorm(
  l_mu = l_mu, 
  l_cvm = l_cvm, 
  l_w = l_w, 
  class_fun = decision)



grid.arrange(
  dataset_linear$border_plot, 
  dataset_normal_dist$border_plot, 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 

```{r}
# define workflows

### Decision tree

# 1. specify the model

decision_tree_rpart_spec <-
  decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
lda_wflow <- 
  workflow() %>% 
  add_model(decision_tree_rpart_spec ) %>% 
  add_recipe(preprocess)


### Boosted tree

# 1. specify the model
boost_tree_C5.0_spec <-
  boost_tree() %>%
  set_engine('C5.0')%>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
log_wflow <- 
  workflow() %>% 
  add_model(boost_tree_C5.0_spec) %>% 
  add_recipe(preprocess)

```


```{r}

data <- list(dataset_linear, dataset_normal_dist)
workflows <- list(boosting = log_wflow, dtree = lda_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_boosting <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$boosting)

metrics_ds2_boosting <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$boosting)


## dtree model

metrics_ds1_dtree <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$dtree)

metrics_ds2_dtree <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$dtree)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_boosting <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$boosting, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_boosting <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$boosting, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_dtree <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$dtree, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_dtree <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$dtree, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```

The plots below show the resulting decision boundaries

```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$boosting,
  compare_fit$plots$model_decision$dataset1$dtree,
  compare_fit$plots$model_decision$dataset2$boosting,
  compare_fit$plots$model_decision$dataset2$dtree,
  nrow = 2,
  top = "Aplying a boostingistic model to both datasets",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border. Visually is possible to conclude that the boostingistic model fits better a linear  frontier while struggling when facing a curved frontier",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```

The plots shows the evolution of key metrics over crossvalidation trainning


```{r}

grid.arrange(
  train_metrics_ds1_boosting, 
  train_metrics_ds1_dtree, 
  train_metrics_ds2_boosting,
  train_metrics_ds2_dtree, 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_boosting$cf_plot ,
  metrics_ds1_dtree$cf_plot ,
  metrics_ds2_boosting$cf_plot ,
  metrics_ds2_dtree$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_boosting$roc_curve ,
  metrics_ds1_dtree$roc_curve ,
  metrics_ds2_boosting$roc_curve ,
  metrics_ds2_dtree$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis eu dictum lorem, et placerat dui. Donec porttitor posuere nisi, non tempor tellus ornare ut. Vestibulum vehicula libero eget consequat lobortis. Suspendisse vel arcu et urna iaculis mattis. Nulla ut mattis est. Pellentesque eu eleifend augue. Maecenas placerat tortor tincidunt risus rutrum tincidunt. 