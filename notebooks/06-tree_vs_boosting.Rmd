---
editor_options:
  chunk_output_type: console
---

# Decision tree vs tree boosting


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

> Boosting works by growing trees sequentially, each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling, instead each tree fit on a modified version of the original dataset. (@James2013)

When compared to a simple decision tree we expect that a boosting model will be able to tackle more complex problems. We previously demonstrated that decesion trees strugle in the case of perfectly linear boundary. How will a boosted tree performe? Therefore we generated the following datasets:


1. A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs above $X_{1} - X_{2} = 0$ are classified as **1** and **0** if below. In other words, we defined a hyperplane through the data and defined each class based on each point position towards that hyper plane.

> @James2013 In a p-dimensional space, a *hyperplane* is a flat affline subspace of dimension p-1. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace-in other words, a line. In two dimensions the hyperplane is defined as 

$$\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} = 0$$


2. A dataset of 1000 pair of (x1,x2) observations from 3 multivariable normal distribution each representing a class. The dataset is perfectly balanced;

```{r}

## A pure linear boundary

decision <- function(x1, x2){
  res <- ifelse(x2 >= x1, 1, 0)
  return(res)
}

dataset_linear <- dataset_gen_unif(
  class_fun = decision, 
  size = 1000)

## A dataset with a multivariable normal distribution and similar covariance matrix

l_mu <- list(
  "g1" = c(6,3), 
  "g2" = c(3,6),
  "g3" = c(4,4)
  )

l_cvm <- list( 
  "covg1" = matrix(c(1,0.4,0.4,1),2,2),
  "covg2" = matrix(c(1,0,0,1),2,2),
  "covg3" = matrix(c(1,-0.4,-0.4,1),2,2)
  )

l_w <- list(
  "wg1" = 1/3, 
  "wg2" = 1/3,
  "wg3" = 1/3
  )


decision <- function(x1,x2){
  
  l_mu <- list(
    "g1" = c(6,3), 
    "g2" = c(3,6),
    "g3" = c(4,4)
    )
  
  l_cvm <- list( 
    "covg1" = matrix(c(1,0.4,0.4,1),2,2),
    "covg2" = matrix(c(1,0,0,1),2,2),
    "covg3" = matrix(c(1,-0.4,-0.4,1),2,2)
    )
  
  px_0 <- dmvnorm(c(x1,x2), mean = l_mu[[1]], sigma = l_cvm[[1]]) 
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu[[2]], sigma = l_cvm[[2]]) 
  px_2 <- dmvnorm(c(x1,x2), mean = l_mu[[3]], sigma = l_cvm[[3]])
  
  
  g <- case_when(
    
    px_0 > px_1 & px_0 > px_2 ~ 0,
    px_1 > px_0 & px_1 > px_2 ~ 1,
    px_2 > px_0 & px_2 > px_1 ~ 2,
    TRUE ~ 2
  )
  
  return(g)
}

dataset_normal_dist <- dataset_gen_mvnorm(
  l_mu = l_mu, 
  l_cvm = l_cvm, 
  l_w = l_w, 
  class_fun = decision)



grid.arrange(
  dataset_linear$border_plot + labs(subtitle ="Linear border", color = ""), 
  dataset_normal_dist$border_plot + labs(subtitle ="Multiclass", color = ""), 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting

The following workflow (using @R-tidymodels) was executed in order to fit and evaluate each model given the above defined datasets:

1. Train - Test split by 80% Train - 20% Test,

2. Define 10 random folds splitting Train and Validation,

3. Fit models to each fold,

4. Calculate Fold metrics,

5. Fit to all train data and extract model,

6. Create plots with estimated decision boundaries

The decision tree is fitted using the following default parameters:

 - Tree depth = 30
 - Minimal Node size = 2
 - Cost complexity = 0.01

Boosted tree:

- trials: 15 => Number of trees
- shinkrage: 0.25

> Attention: adaBoost package is not implemented out the box in Parsnip, the fitting package and approach used on this project. Instead of going for a manual implementation we opted to use the C5.0 implementation on boosted trees which very mush resembles the same approach as adaboost.

```{r}
# define workflows

### Decision tree

# 1. specify the model

decision_tree_rpart_spec <-
  decision_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
dtree_wflow <- 
  workflow() %>% 
  add_model(decision_tree_rpart_spec ) %>% 
  add_recipe(preprocess)


### Boosted tree

# 1. specify the model
boost_tree_C5.0_spec <-
  boost_tree() %>%
  set_engine('C5.0')%>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
boosted_wflow <- 
  workflow() %>% 
  add_model(boost_tree_C5.0_spec) %>% 
  add_recipe(preprocess)

```


```{r}

data <- list(dataset_linear, dataset_normal_dist)
workflows <- list(boosting = boosted_wflow, dtree = dtree_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_boosting <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$boosting)

metrics_ds2_boosting <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$boosting)


## dtree model

metrics_ds1_dtree <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$dtree)

metrics_ds2_dtree <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$dtree)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_boosting <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$boosting, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_boosting <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$boosting, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_dtree <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$dtree, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_dtree <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$dtree, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```


```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$boosting + labs(subtitle = "Boosting"),
  compare_fit$plots$model_decision$dataset1$dtree + labs(subtitle = "Decision tree"),
  compare_fit$plots$model_decision$dataset2$boosting,
  compare_fit$plots$model_decision$dataset2$dtree,
  nrow = 2,
  top = "Decision Tree vs Boosting",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border.",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```


```{r}

grid.arrange(
  train_metrics_ds1_boosting + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds1_dtree + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds2_boosting + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)),
  train_metrics_ds2_dtree + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_boosting$cf_plot ,
  metrics_ds1_dtree$cf_plot ,
  metrics_ds2_boosting$cf_plot ,
  metrics_ds2_dtree$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_boosting$roc_curve ,
  metrics_ds1_dtree$roc_curve ,
  metrics_ds2_boosting$roc_curve ,
  metrics_ds2_dtree$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

Undoubtedly the boosted approach outperformed a decision tree when a linear border scenario (although still behind lda or logistic). 


```{r}

knitr::kable(
  
  data.frame(
    ds2_boosting = metrics_ds2_boosting$auc_roc$.estimate,
    ds2_dtree = metrics_ds2_dtree$auc_roc$.estimate,
    ds1_boosting = metrics_ds1_boosting$auc_roc$.estimate,
    ds1_dtree = metrics_ds1_dtree$auc_roc$.estimate
    )
  
)

```