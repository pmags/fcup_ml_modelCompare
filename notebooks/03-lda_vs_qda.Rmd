---
editor_options:
  chunk_output_type: console
---

<<<<<<< HEAD:notebooks/3-lda_vs_qda.Rmd
# Linear Discriminante Analysis vs Quadratic Discriminante Analysis
=======
# Linear Discriminante Analysis vs Quadratic Discriminate Analysis
>>>>>>> c5a570a8d341739b8e44d012d8ee61d03e26740d:notebooks/03-lda_vs_qda.Rmd


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(tidymodels)
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```

## Dataset definition  
## Dataset definition  
Linear Discriminant Analysis
LDA assumes normally distributed data,a class-specific mean vector and assumes a common covariance matrix. So, a covariance matrix that is common to all classes in a data set.
When these assumptions hold, then LDA approximates the Bayes classifier very closely and the discriminant function produces a linear decision boundary. 

Quadratic Discriminant Analysis

QDA assumes a normal distribution (same as LDA) and assumes that each class has its own covariance matrix (different from LDA).
When these assumptions hold, QDA approximates the Bayes classifier very closely and the discriminant function produces a quadratic decision boundary.

So, we construct two datasets:
1. LDA dataset: The dataset have 1000 samples and two variables that have very similar covariance between them.

2. QDA dataset: The dataset have 1000 samples and two variables that follows a different covariance between them.

Therefore we are expecting that the LDA models shows better results with the the LDA dataset, even though the QDA do not show a huge diference in this dataset. The QDA model will work better in the QDA dataset, because the assumptions of LDA model do not hold in this dataset.


```{r}
# LDA

l_mu_1 <- list(
  "g1" = c(3,4), 
  "g2" = c(1,1)
  )

l_cvm_1 <- list( 
  "covg1" = matrix(c(1, 0, 0, 1),2,2),
  "covg2" = matrix(c(1, 0, 0, 1),2,2)
  )


l_w_1 <- list(
  "wg1" = 1/2, 
  "wg2" = 1/2
  )


decision_1 <- function(x1, x2, l_mu_1, l_cvm_1){

l_mu_1 <- list(
  "g1" = c(3,4), 
  "g2" = c(1,1)
  )

l_cvm_1 <- list( 
  "covg1" = matrix(c(1, 0, 0, 1),2,2),
  "covg2" = matrix(c(0.5,-0.4,-0.4,0.5),2,2)
  )


l_w_1 <- list(
  "wg1" = 1/2, 
  "wg2" = 1/2
  )

  px_0 <- dmvnorm(c(x1,x2), mean = l_mu_1[[1]], sigma = l_cvm_1[[1]]) * 1/2
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu_1[[2]], sigma = l_cvm_1[[2]]) * 1/2
 
  
  g <- case_when(
    
    px_0 > px_1 ~ 0,
    px_1 > px_0 ~ 1,
    TRUE ~ 1
  )
  
  return(g)
}

dataset_lda <- dataset_gen_mvnorm(l_mu_1, l_cvm_1, l_w_1, class_fun = decision_1, n_g = 2)

```

```{r}

l_mu_2 <- list(
  "g1" = c(5,5), 
  "g2" = c(3,4))

l_cvm_2 <- list( 
  "covg1" = matrix(c(3, 0, 0, 3),2,2),
  "covg2" = matrix(c(0.1,0,0,0.1),2,2)
  )

l_w_2 <- list(
  "wg1" = 1/2, 
  "wg2" = 1/2
  )


decision_2 <- function(x1,x2, l_mu_2, l_cvm_2){

l_mu_2 <- list(
  "g1" = c(5,5), 
  "g2" = c(3,4))

l_cvm_2 <- list( 
  "covg1" = matrix(c(3, 0, 0, 3),2,2),
  "covg2" = matrix(c(0.1,0,0,0.1),2,2)
  )


  px_0 <- dmvnorm(c(x1,x2), mean = l_mu_2[[1]], sigma = l_cvm_2[[1]])  
  px_1 <- dmvnorm(c(x1,x2), mean = l_mu_2[[2]], sigma = l_cvm_2[[2]]) 
 
  g <- case_when(
    
    px_0 > px_1 ~ 0,
    px_1 > px_0 ~ 1,
    TRUE ~ 1
  )
  
  return(g)
}

dataset_qda <- dataset_gen_mvnorm(l_mu_2, l_cvm_2, l_w_2, class_fun = decision_2, n_g = 2)

```

```{r}

grid.arrange(
  dataset_lda$border_plot, 
  dataset_qda$border_plot, 
  nrow = 1,
  top = "Optimal decision border")

```
# define workflows

### LDA
```{r}
# 1. specify the model

discrim_linear_MASS_spec <-
  discrim_linear() %>%
  set_engine('MASS') %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_lda$dataset)
  
# 3, Buildworkflow
discrim_linear_MASS_wflow <- 
  workflow() %>% 
  add_model(discrim_linear_MASS_spec) %>% 
  add_recipe(preprocess)

```
### QDA
```{r}
# 1. specify the model
discrim_quad_MASS_spec <-
  discrim_quad() %>%
  set_engine('MASS') %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_qda$dataset)
  
# 3, Buildworkflow
discrim_quad_MASS_wflow <- 
  workflow() %>% 
  add_model(discrim_quad_MASS_spec) %>% 
  add_recipe(preprocess)

```

###Fitting
```{r}

data <- list(dataset_lda, dataset_qda)
workflows <- list(LDA = discrim_linear_MASS_wflow , QDA = discrim_quad_MASS_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results
```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## LDA

metrics_ds1_LDA <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$LDA)

metrics_ds2_LDA <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$LDA)


## QDA

metrics_ds1_QDA <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$QDA)

metrics_ds2_QDA <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$QDA)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_LDA <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$LDA, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_LDA <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$LDA, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_QDA <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$QDA, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_QDA <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$QDA, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```

The plots below show the resulting decision boundaries

```{r}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$LDA + labs(title = "dataset 1 - LDA"),
  compare_fit$plots$model_decision$dataset1$QDA + labs(title = "dataset 1 - QDA"),
  compare_fit$plots$model_decision$dataset2$LDA + labs(title = "dataset 2 - LDA"),
  compare_fit$plots$model_decision$dataset2$QDA + labs(title = "dataset 2 - QDA"),
  nrow = 2,
  top = "Aplying a svm_linearistic model to both datasets",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border. Visually is possible to conclude that the svm_linearistic model fits better a linear  frontier while struggling when facing a curved frontier",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```

The plots shows the evolution of key metrics over crossvalidation trainning


```{r}

grid.arrange(
  train_metrics_ds1_LDA + labs(title = "dataset 1 - LDA"), 
  train_metrics_ds1_QDA + labs(title = "dataset 1 - QDA"), 
  train_metrics_ds2_LDA + labs(title = "dataset 2 - LDA"),
  train_metrics_ds2_QDA + labs(title = "dataset 2 - QDA"), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_LDA$cf_plot + labs(title = "dataset 1 - LDA") ,
  metrics_ds1_QDA$cf_plot + labs(title = "dataset 1 - QDA") ,
  metrics_ds2_LDA$cf_plot + labs(title = "dataset 2 - LDA"),
  metrics_ds2_QDA$cf_plot + labs(title = "dataset 2 - QDA") ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_LDA$roc_curve + labs(title = "dataset 1 - LDA"),
  metrics_ds1_QDA$roc_curve + labs(title = "dataset 1 - QDA"),
  metrics_ds2_LDA$roc_curve + labs(title = "dataset 2 - LDA"),
  metrics_ds2_QDA$roc_curve + labs(title = "dataset 2 - QDA"),
  nrow = 2,
  top = "ROC curves"
)

```
## Conclusion

Given the strong assumptions of LDA models on the covariance of the classes it is easy to understand that results. The QDA relaxation in relation to this allow the good performance in the QDA dataset, but also a good result in the LDA dataset.
