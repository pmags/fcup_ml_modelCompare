---
editor_options:
  chunk_output_type: console
---

# SVM Radial vs SVM linear


```{r  echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}

knitr::opts_chunk$set(fig.align="center", echo=FALSE, fig.show = "hold")
source("./scripts/helper.R", local = TRUE, encoding = "UTF-8")

```

```{r include=FALSE}

# libraries
library(workflowsets)
library(gridExtra)

# global seed for reproducibility
set.seed(123)

```


## Dataset definition  

We expect support vector machines with a linear kernel to perform very weel in case where the classification border follows a perfect line. On the other hand, non linear boundaries will lead to poor performance. For this example we set a very extrem case where a boundary is a perfectly centered circle.

1. A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs above $X_{1} - X_{2} = 0$ are classified as **1** and **0** if below. In other words, we defined a hyperplane through the data and defined each class based on each point position towards that hyper plane.

> @James2013 In a p-dimensional space, a *hyperplane* is a flat affline subspace of dimension p-1. For instance, in two dimensions, a hyperplane is a flat one-dimensional subspace-in other words, a line. In two dimensions the hyperplane is defined as 

$$\beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2} = 0$$

2. A dataset of 1000 experiments extracting, with repetition, pair of real numbers 1:100 all of them with equal probability (uniform distribution). Pairs inside the circle $(X_{1}-5)^2 + (5 - X_{2})^2 = 4$ are classified as **1** and **0** if below. 


```{r}

## A pure linear boundary

decision <- function(x1, x2){
  res <- ifelse(x2 >= x1, 1, 0)
  return(res)
}

dataset_linear <- dataset_gen_unif(
  class_fun = decision, 
  size = 1000)



## A circular boundary

decision <- function(x1, x2){
  res <- ifelse((-5 + x1)^2 + (5 - x2)^2 > 4, 1, 0)
  return(res)
}

dataset_radial <- dataset_gen_unif(
  class_fun = decision, 
  size = 1000)


grid.arrange(
  dataset_linear$border_plot + labs(subtitle ="Linear border", color = "" ), 
  dataset_radial$border_plot + labs(subtitle ="Circle border", color = "" ), 
  nrow = 1,
  top = "Optimal decision border")

```


## Model fitting

The following workflow (using @R-tidymodels) was executed in order to fit and evaluate each model given the above defined datasets:

1. Train - Test split by 80% Train - 20% Test,

2. Define 10 random folds splitting Train and Validation,

3. Fit models to each fold,

4. Calculate Fold metrics,

5. Fit to all train data and extract model,

6. Create plots with estimated decision boundaries 


```{r}
# define workflows

### SVM linear

# 1. specify the model

svm_linear_spec <-
  svm_linear(cost = 1) %>% 
  set_engine("kernlab") %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
svm_linear_wflow <- 
  workflow() %>% 
  add_model(svm_linear_spec) %>% 
  add_recipe(preprocess)


### SVM radial

# 1. specify the model
svm_rbf_kernlab_spec <-
  svm_rbf(cost = 1) %>%
  set_engine('kernlab') %>%
  set_mode('classification')


# 2. preprocessing 
preprocess <- 
  recipe(g ~ x1 + x2 , data = dataset_linear$dataset)
  
# 3, Buildworkflow
svm_radial_wflow <- 
  workflow() %>% 
  add_model(svm_rbf_kernlab_spec) %>% 
  add_recipe(preprocess)

```


```{r}

data <- list(dataset_linear, dataset_radial)
workflows <- list(svm_linear = svm_linear_wflow, svm_radial = svm_radial_wflow)

compare_fit <- model_fit_compare(data = data, workflows = workflows)

```


## Compare results

```{r}

## Metrics logistic model:
## using yardstick package for confusion matrix and accuracy
## using yardstick for roc curve and plots

## Logistic model

metrics_ds1_svm_linear <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$svm_linear)

metrics_ds2_svm_linear <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$svm_linear)


## svm_radial model

metrics_ds1_svm_radial <- 
  model_metrics(test_data = compare_fit$models$test$dataset1, 
                model = compare_fit$models$models$dataset1$svm_radial)

metrics_ds2_svm_radial <- 
  model_metrics(test_data = compare_fit$models$test$dataset2, 
                model = compare_fit$models$models$dataset2$svm_radial)

```


```{r}

# logistic regression metrics during training

train_metrics_ds1_svm_linear <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$svm_linear, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_svm_linear <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$svm_linear, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()


# knn  performance during training

train_metrics_ds1_svm_radial <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset1$svm_radial, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

train_metrics_ds2_svm_radial <- 
  ggplot(
    collect_metrics(
      compare_fit$models$fit_resample_train$dataset2$svm_radial, 
      summarize = FALSE), 
    aes(x = id, y = .estimate, color= .metric)) + 
  geom_point() +
  theme_bw()

```

The plots below show the resulting decision boundaries

```{r message=FALSE, warning=FALSE}

## compare results

grid.arrange(
  compare_fit$plots$model_decision$dataset1$svm_linear + labs(subtitle = "Linear decision"),
  compare_fit$plots$model_decision$dataset1$svm_radial + labs(subtitle = "Radial decision"),
  compare_fit$plots$model_decision$dataset2$svm_linear,
  compare_fit$plots$model_decision$dataset2$svm_radial,
  nrow = 2,
  top = "SVM linear vs SVM radial",
  bottom = grid::textGrob(
    "Colored region represents the decision area of the model. Purple line represents the decision border.",
    gp = grid::gpar(fontface = 3, fontsize = 9)
    )
  )

```



```{r}

grid.arrange(
  train_metrics_ds1_svm_linear + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds1_svm_radial + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  train_metrics_ds2_svm_linear + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)),
  train_metrics_ds2_svm_radial + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)), 
  nrow = 2,
  top = "Metrics over different folds",
  bottom = grid::textGrob(
      "Right plots reflect trainning over linear border dataset and left plots reflect a quadractic border",
      gp = grid::gpar(fontface = 3, fontsize = 9)
      )
)

```


The plots show the metrics between different models

```{r}

grid.arrange(
  metrics_ds1_svm_linear$cf_plot ,
  metrics_ds1_svm_radial$cf_plot ,
  metrics_ds2_svm_linear$cf_plot ,
  metrics_ds2_svm_radial$cf_plot ,
  nrow = 2,
  top = "Confusion Matrix"
)


```


```{r}

grid.arrange(
  metrics_ds1_svm_linear$roc_curve ,
  metrics_ds1_svm_radial$roc_curve ,
  metrics_ds2_svm_linear$roc_curve ,
  metrics_ds2_svm_radial$roc_curve ,
  nrow = 2,
  top = "ROC curves"
)

```


## Conclusion

As expected, a linear kernel performed very poorly in the case where a non linear border existed. So poorly that in our experiment it couldn't even fit a model to the data since it couldn't find a single model which minimized the loss function.

```{r}

knitr::kable(
  
  data.frame(
    ds2_svm_linear = metrics_ds2_svm_linear$auc_roc$.estimate,
    ds2_svm_radial = metrics_ds2_svm_radial$auc_roc$.estimate,
    ds1_svm_linear = metrics_ds1_svm_linear$auc_roc$.estimate,
    ds1_svm_radial = metrics_ds1_svm_radial$auc_roc$.estimate
    )
)

```
